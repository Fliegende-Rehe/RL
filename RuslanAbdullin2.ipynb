{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "rl",
   "language": "python",
   "display_name": "RL"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL Final Project\n",
    "\n",
    "Now it's finally time to put into use what we have learned so far in this course!\n",
    "\n",
    "The aim of this project is to assess your practical knowledge in Reinforcement Learning.\n",
    "\n",
    "your project consist of 2 parts. you will get the chance to work with 2 different environment.\n"
   ],
   "metadata": {
    "id": "AQWaHq-KLtHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Atari Game Pong"
   ],
   "metadata": {
    "id": "TdA4hr4kR33L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"zzzzzzzzzzzzzzzzzc\"/>"
   ],
   "metadata": {
    "id": "bS8EasNeaVx-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[Pong](https://www.gymlibrary.dev/environments/atari/pong/)** is a famus atari game that almost all of us have played it at least once!\n",
    "The goal of this task is to get engage with **gym** library and use Deep Reinforcement Learning to train an agent which can actually play this game!"
   ],
   "metadata": {
    "id": "lD3mZJkBWGxp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install ALE\n",
    "# !pip install gym\n",
    "# !pip install opencv-python\n",
    "#\n",
    "# !pip install \"tensorflow==2.10\"\n",
    "# !pip install \"tensorflow-gpu==2.10\"\n",
    "#\n",
    "# !pip install tqdm\n",
    "# !pip install jdc\n",
    "#\n",
    "!pip list"
   ],
   "metadata": {
    "id": "nq9-gTd4Whko"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- -----------\n",
      "absl-py                      1.4.0\n",
      "Ale                          0.8.4\n",
      "ale-py                       0.8.1\n",
      "anyio                        3.5.0\n",
      "appdirs                      1.4.4\n",
      "argon2-cffi                  21.3.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "asttokens                    2.0.5\n",
      "astunparse                   1.6.3\n",
      "attrs                        22.1.0\n",
      "AutoROM                      0.4.2\n",
      "AutoROM.accept-rom-license   0.6.1\n",
      "Babel                        2.11.0\n",
      "backcall                     0.2.0\n",
      "beautifulsoup4               4.11.1\n",
      "bleach                       4.1.0\n",
      "brotlipy                     0.7.0\n",
      "cachetools                   5.3.0\n",
      "certifi                      2023.5.7\n",
      "cffi                         1.15.1\n",
      "charset-normalizer           2.0.4\n",
      "click                        8.1.3\n",
      "cloudpickle                  2.2.1\n",
      "colorama                     0.4.6\n",
      "comm                         0.1.2\n",
      "cryptography                 38.0.4\n",
      "debugpy                      1.5.1\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "docker-pycreds               0.4.0\n",
      "entrypoints                  0.4\n",
      "executing                    0.8.3\n",
      "fastjsonschema               2.16.2\n",
      "flatbuffers                  23.5.9\n",
      "flit_core                    3.6.0\n",
      "gast                         0.4.0\n",
      "gitdb                        4.0.10\n",
      "GitPython                    3.1.31\n",
      "google-auth                  2.18.1\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.54.2\n",
      "gym                          0.26.2\n",
      "gym-notices                  0.0.8\n",
      "h5py                         3.8.0\n",
      "idna                         3.4\n",
      "importlib-resources          5.12.0\n",
      "ipykernel                    6.19.2\n",
      "ipython                      8.8.0\n",
      "ipython-genutils             0.2.0\n",
      "ipywidgets                   7.6.5\n",
      "jdc                          0.0.9\n",
      "jedi                         0.18.1\n",
      "Jinja2                       3.1.2\n",
      "json5                        0.9.6\n",
      "jsonschema                   4.16.0\n",
      "jupyter                      1.0.0\n",
      "jupyter_client               7.4.8\n",
      "jupyter-console              6.4.4\n",
      "jupyter_core                 5.1.1\n",
      "jupyter-server               1.23.4\n",
      "jupyterlab                   3.5.3\n",
      "jupyterlab-pygments          0.1.2\n",
      "jupyterlab_server            2.16.5\n",
      "jupyterlab-widgets           1.0.0\n",
      "keras                        2.10.0\n",
      "Keras-Preprocessing          1.1.2\n",
      "libclang                     16.0.0\n",
      "lxml                         4.9.1\n",
      "Markdown                     3.4.3\n",
      "MarkupSafe                   2.1.1\n",
      "matplotlib-inline            0.1.6\n",
      "mistune                      0.8.4\n",
      "nbclassic                    0.4.8\n",
      "nbclient                     0.5.13\n",
      "nbconvert                    6.5.4\n",
      "nbformat                     5.7.0\n",
      "nest-asyncio                 1.5.6\n",
      "notebook                     6.5.2\n",
      "notebook_shim                0.2.2\n",
      "numpy                        1.24.3\n",
      "oauthlib                     3.2.2\n",
      "opencv-python                4.7.0.72\n",
      "opt-einsum                   3.3.0\n",
      "packaging                    22.0\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "pathtools                    0.1.2\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       9.5.0\n",
      "pip                          22.3.1\n",
      "platformdirs                 2.5.2\n",
      "ply                          3.11\n",
      "prometheus-client            0.14.1\n",
      "prompt-toolkit               3.0.20\n",
      "protobuf                     3.20.3\n",
      "psutil                       5.9.0\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.0\n",
      "pyasn1-modules               0.3.0\n",
      "pycparser                    2.21\n",
      "Pygments                     2.11.2\n",
      "pyOpenSSL                    22.0.0\n",
      "PyQt5                        5.15.7\n",
      "PyQt5-sip                    12.11.0\n",
      "pyrsistent                   0.18.0\n",
      "PySocks                      1.7.1\n",
      "python-dateutil              2.8.2\n",
      "pytz                         2022.7\n",
      "pywin32                      305.1\n",
      "pywinpty                     2.0.2\n",
      "PyYAML                       6.0\n",
      "pyzmq                        23.2.0\n",
      "qtconsole                    5.4.0\n",
      "QtPy                         2.2.0\n",
      "requests                     2.28.1\n",
      "requests-oauthlib            1.3.1\n",
      "rsa                          4.9\n",
      "Send2Trash                   1.8.0\n",
      "sentry-sdk                   1.24.0\n",
      "setproctitle                 1.3.2\n",
      "setuptools                   65.6.3\n",
      "sip                          6.6.2\n",
      "six                          1.16.0\n",
      "smmap                        5.0.0\n",
      "sniffio                      1.2.0\n",
      "soupsieve                    2.3.2.post1\n",
      "stack-data                   0.2.0\n",
      "tensorboard                  2.10.1\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.10.0\n",
      "tensorflow-estimator         2.10.0\n",
      "tensorflow-gpu               2.10.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.3.0\n",
      "terminado                    0.17.1\n",
      "tinycss2                     1.2.1\n",
      "toml                         0.10.2\n",
      "tomli                        2.0.1\n",
      "tornado                      6.2\n",
      "tqdm                         4.65.0\n",
      "traitlets                    5.7.1\n",
      "typing_extensions            4.4.0\n",
      "urllib3                      1.26.14\n",
      "wandb                        0.15.3\n",
      "wcwidth                      0.2.5\n",
      "webencodings                 0.5.1\n",
      "websocket-client             0.58.0\n",
      "Werkzeug                     2.3.4\n",
      "wheel                        0.37.1\n",
      "widgetsnbextension           3.5.2\n",
      "win-inet-pton                1.1.0\n",
      "wincertstore                 0.2\n",
      "wrapt                        1.15.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Importing Libraries:** The necessary libraries and modules are imported, including gym for the game environment, random for random actions, warnings for suppressing warnings, numpy for numerical operations, tensorflow for building and training neural networks, PIL for image preprocessing, collections for deque (double-ended queue) data structure, IPython for capturing output, and tqdm for displaying progress bars."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from IPython.utils import io\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "id": "p6T-o8XRQ54i",
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:48.542480100Z",
     "start_time": "2023-05-27T04:28:45.563619800Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ignoring Warnings:** The warnings.filterwarnings('ignore') statement is used to suppress warnings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:48.558133200Z",
     "start_time": "2023-05-27T04:28:48.542480100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Listing GPU Devices:** tf.config.list_physical_devices('GPU') lists the available GPU devices. This line is likely used to check if a GPU is available for acceleration but does not store or use the output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:48.620697200Z",
     "start_time": "2023-05-27T04:28:48.604975300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training Configuration:** Several constants and configuration parameters are defined, such as TRAIN (a boolean flag indicating whether to train the agent), GAME_NAME (the name of the game environment), MODEL_PATH (the path to save the trained model), INPUT_SHAPE (the shape of input frames), BATCH_SIZE (the size of training batches), MEMORY_SIZE (the size of the replay buffer), TARGET_UPDATE_FREQ (the frequency of updating the target network), GAMMA (the discount factor), EPSILON (the exploration rate), MIN_EPSILON (the minimum exploration rate), and EPSILON_DECAY (the decay rate for exploration)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "GAME_NAME = 'ALE/Pong-v5'\n",
    "MODEL_PATH = './models/pong_model.h5'\n",
    "INPUT_SHAPE = (84, 84, 1)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "\n",
    "GAMMA = 0.95\n",
    "EPSILON = 1.0\n",
    "MIN_EPSILON = 0.1\n",
    "EPSILON_DECAY = 0.999"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:48.667504800Z",
     "start_time": "2023-05-27T04:28:48.636257100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Prioritized Replay Buffer:** The PrioritizedReplayBuffer class implements a prioritized replay buffer for storing and sampling experiences. Experiences are stored in a deque with limited size, and priorities are stored in a separate deque. Experiences can be appended, sampled, and priorities can be updated."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, size, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.priorities = deque(maxlen=size)\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(self.max_priority)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        probs = np.array(self.priorities) ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        beta = self.beta_start + (1 - self.beta_start) * (self.frame / self.beta_frames)\n",
    "        self.frame += 1\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        experiences = [self.buffer[i] for i in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return experiences, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, indices, errors, absolute_error=1e-5):\n",
    "        priorities = self.priorities\n",
    "        max_priority = self.max_priority\n",
    "        for i, error in zip(indices, errors):\n",
    "            priority = np.max(np.abs(error)) + absolute_error\n",
    "            priorities[i] = priority\n",
    "            max_priority = max(max_priority, priority)\n",
    "        self.max_priority = max_priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:48.667504800Z",
     "start_time": "2023-05-27T04:28:48.667504800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**DQN Model:** The DQNModel class defines the architecture of the DQN model using convolutional and dense layers. The call method specifies the forward pass of the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class DQNModel(tf.keras.Model):\n",
    "    def __init__(self, action_size, act='relu'):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, 8, 4, activation=act)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 4, 2, activation=act)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(64, 3, 1, activation=act)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(512, activation=act)\n",
    "        self.outputs = tf.keras.layers.Dense(action_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        return self.outputs(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:48.683103Z",
     "start_time": "2023-05-27T04:28:48.667504800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**DQN Agent:** The DQNAgent class represents the DQN agent. It initializes the agent with the state and action sizes, creates a replay buffer, sets the exploration rate, builds the model and target model using the DQNModel class, initializes an optimizer, and defines methods for updating the target model, choosing actions, training a step, running an episode, remembering experiences, loading and saving model weights."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
    "        self.epsilon = EPSILON\n",
    "        self.model = DQNModel(self.action_size)\n",
    "        self.target_model = DQNModel(self.action_size)\n",
    "        self.update_target_model()\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, states, target_f, weights):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = tf.recompute_grad(self.model)(states / 255.0)\n",
    "            loss = tf.keras.losses.MSE(target_f, predictions)\n",
    "            loss = tf.reduce_mean(loss * weights)\n",
    "        grads = tape.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "\n",
    "    def run_episode(self):\n",
    "        experiences, indices, weights = self.memory.sample(BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        next_states = np.concatenate(next_states, axis=0)\n",
    "\n",
    "        target_q_values = self.target_model.predict(next_states / 255.0)\n",
    "        online_q_values = self.model.predict(next_states / 255.0)\n",
    "        best_actions = np.argmax(online_q_values, axis=1)\n",
    "\n",
    "        targets = np.array(rewards) + (1 - np.array(dones)) * GAMMA * target_q_values[\n",
    "            np.arange(BATCH_SIZE), best_actions]\n",
    "\n",
    "        target_f = self.model.predict(states / 255.0)\n",
    "        target_f[np.arange(BATCH_SIZE), np.array(actions)] = targets\n",
    "\n",
    "        errors = np.abs(self.model.predict(states / 255.0) - target_f)\n",
    "        self.memory.update_priorities(indices, errors)\n",
    "\n",
    "        if self.epsilon > MIN_EPSILON:\n",
    "            self.epsilon *= EPSILON_DECAY\n",
    "\n",
    "        self.train_step(states, target_f, weights)\n",
    "\n",
    "        self.update_target_model()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:48.698781600Z",
     "start_time": "2023-05-27T04:28:48.683103Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Frame Preprocessing:** The preprocess_frame function takes a frame as input, rescales it, converts it to grayscale, resizes it to the desired input shape, and returns the preprocessed frame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame[0].astype(np.float32) / 255.0\n",
    "    if len(frame.shape) > 2 and frame.shape[-1] == 3:\n",
    "        frame = np.dot(frame[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    frame = Image.fromarray(frame).resize((INPUT_SHAPE[0], INPUT_SHAPE[1]))\n",
    "    frame = np.expand_dims(frame, axis=2)\n",
    "    return frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:48.714354Z",
     "start_time": "2023-05-27T04:28:48.698781600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training Function:** The train function trains the DQN agent for a specified number of episodes. It creates the game environment, initializes the agent, loads model weights (if available), iterates over episodes, resets the environment, preprocesses the initial state, chooses actions, takes steps, remembers experiences, runs episodes, updates the target model, and displays progress."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train(episodes):\n",
    "    env = gym.make(GAME_NAME)\n",
    "    state_size = INPUT_SHAPE\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    dummy_data = np.zeros((1,) + INPUT_SHAPE)\n",
    "    with io.capture_output() as captured:\n",
    "        agent.model.predict(dummy_data)\n",
    "        agent.target_model.predict(dummy_data)\n",
    "\n",
    "    agent.load(MODEL_PATH)\n",
    "\n",
    "    bar_format = 'Training: {percentage:3.0f}% |{bar}| Elapsed: {elapsed} Remaining: {remaining}{postfix}'\n",
    "    training_pbar = tqdm(total=episodes, bar_format=bar_format, unit='episode')\n",
    "\n",
    "    best_total_reward = -np.inf\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_frame(state)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with io.capture_output() as captured:\n",
    "                action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state = preprocess_frame(next_state)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if total_reward > best_total_reward:\n",
    "                    print(f\"New best total reward {total_reward}, saving model weights.\")\n",
    "                    best_total_reward = total_reward\n",
    "                    agent.save(MODEL_PATH)\n",
    "\n",
    "        if len(agent.memory) >= BATCH_SIZE and len(agent.memory) >= BATCH_SIZE:\n",
    "            with io.capture_output() as captured:\n",
    "                agent.run_episode()\n",
    "\n",
    "        if e % TARGET_UPDATE_FREQ == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        training_pbar.set_postfix_str(f'Reward: {total_reward}')\n",
    "        training_pbar.update(1)\n",
    "\n",
    "    training_pbar.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T04:28:48.730031500Z",
     "start_time": "2023-05-27T04:28:48.714354Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training Execution:**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0% |          | Elapsed: 00:00 Remaining: ?",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79fe377a12ed45ae90b45cd58f7c39cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best total reward -21.0, saving model weights.\n",
      "New best total reward -20.0, saving model weights.\n",
      "New best total reward -16.0, saving model weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x000002308A5193F0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\weakref.py\", line 371, in remove\n",
      "    self = selfref()\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    train(episodes=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T04:41:12.756019800Z",
     "start_time": "2023-05-27T04:28:48.730031500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Playing the game:** use the trained DQN agent. It creates the Gym environment, initializes the agent, loads the model weights, and interacts with the environment based on the agent's actions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def play_with_model():\n",
    "    env = gym.make(GAME_NAME, render_mode='human')\n",
    "    state_size = INPUT_SHAPE\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.load(MODEL_PATH)\n",
    "\n",
    "    state = preprocess_frame(env.reset())\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        with io.capture_output() as captured:\n",
    "            action = agent.choose_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = preprocess_frame(next_state)\n",
    "        state = np.expand_dims(state, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Playing execution:**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play_with_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: Keep in mind that observation space for this environment are frames from environment. Observation space is an image of size (210, 160, 3). so you will need to implement an agent which can process images!(a CNN based agent). \n",
    "\n",
    "Make sure to do perform preprocessing on the frames. For example, you can convert the RBG image to gray. you can use [OpenCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) library to perform resize\\ing, bluring or any applicable filtering on the frames."
   ],
   "metadata": {
    "id": "3CYO6rorZlrC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grading criteria\n",
    "Project: 35 points\n",
    "\n",
    "* Final Viva: 10 points\n",
    "* Implementation: 10 points\n",
    "* Final Report: 15 points\n",
    "\n",
    "For viva you will need to expilictly mention each team member's contribution.\n",
    "\n",
    "You can write your report on this notebook. The report must include visualization of your results. Train your model at least with 2 different sets of hyperparameters and in visualization section compare their output.\n"
   ],
   "metadata": {
    "id": "nUJd6RS3dot5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good Luck!"
   ],
   "metadata": {
    "id": "LjLQtO8UdtEj"
   }
  }
 ]
}
