{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "rl",
   "language": "python",
   "display_name": "RL"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL Final Project\n",
    "\n",
    "Now it's finally time to put into use what we have learned so far in this course!\n",
    "\n",
    "The aim of this project is to assess your practical knowledge in Reinforcement Learning.\n",
    "\n",
    "your project consist of 2 parts. you will get the chance to work with 2 different environment.\n"
   ],
   "metadata": {
    "id": "AQWaHq-KLtHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Atari Game Pong"
   ],
   "metadata": {
    "id": "TdA4hr4kR33L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"zzzzzzzzzzzzzzzzzc\"/>"
   ],
   "metadata": {
    "id": "bS8EasNeaVx-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[Pong](https://www.gymlibrary.dev/environments/atari/pong/)** is a famus atari game that almost all of us have played it at least once!\n",
    "The goal of this task is to get engage with **gym** library and use Deep Reinforcement Learning to train an agent which can actually play this game!"
   ],
   "metadata": {
    "id": "lD3mZJkBWGxp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install ALE\n",
    "# !pip install gym\n",
    "# !pip install opencv-python\n",
    "#\n",
    "# !pip install \"tensorflow==2.10\"\n",
    "# !pip install \"tensorflow-gpu==2.10\"\n",
    "#\n",
    "# !pip install tqdm\n",
    "# !pip install jdc\n",
    "#\n",
    "# !pip list"
   ],
   "metadata": {
    "id": "nq9-gTd4Whko",
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.073321200Z",
     "start_time": "2023-05-26T18:23:53.041993800Z"
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import cv2\n",
    "import jdc\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from IPython.utils import io\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "id": "p6T-o8XRQ54i",
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.106605400Z",
     "start_time": "2023-05-26T18:23:53.057620400Z"
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.106605400Z",
     "start_time": "2023-05-26T18:23:53.075351500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84, 1)\n",
    "TRAIN = True\n",
    "GAME_NAME = 'ALE/Pong-v5'\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 2000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.106605400Z",
     "start_time": "2023-05-26T18:23:53.090992400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.165622Z",
     "start_time": "2023-05-26T18:23:53.106605400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name,\n",
    "            action_space,\n",
    "            gamma=0.95,\n",
    "            epsilon=1.0, epsilon_min=0.01,\n",
    "            epsilon_decay=0.995,\n",
    "            learning_rate=0.001,\n",
    "            episodes=100\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.observation_space = INPUT_SHAPE\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        self.episodes = episodes\n",
    "        self.batch_size = BATCH_SIZE\n",
    "\n",
    "    def build_model(self, pad='same'):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Conv2D(32, (8, 8), strides=4, padding=pad, input_shape=INPUT_SHAPE))\n",
    "        model.add(tf.keras.layers.Conv2D(64, (4, 4), strides=2, padding=pad))\n",
    "        model.add(tf.keras.layers.Conv2D(64, (3, 3), strides=1, padding=pad))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.165622Z",
     "start_time": "2023-05-26T18:23:53.134392200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def preprocess_frame(self, frame):\n",
    "    frame = frame[0]\n",
    "    if len(frame.shape) == 3 and frame.shape[2] == 3:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(frame, (84, 84))\n",
    "    return np.expand_dims(resized, axis=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.165622Z",
     "start_time": "2023-05-26T18:23:53.165622Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def remember(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.181252100Z",
     "start_time": "2023-05-26T18:23:53.165622Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "        return random.randrange(self.action_space)\n",
    "    with io.capture_output() as captured:\n",
    "        act_values = self.model.predict(state.reshape(1, *state.shape))\n",
    "    return np.argmax(act_values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.205587500Z",
     "start_time": "2023-05-26T18:23:53.181252100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def replay(self, batch_size):\n",
    "    minibatch = random.sample(self.memory, batch_size)\n",
    "    states = np.array([x[0] for x in minibatch])\n",
    "    actions = np.array([x[1] for x in minibatch])\n",
    "    rewards = np.array([x[2] for x in minibatch])\n",
    "    next_states = np.array([x[3] for x in minibatch])\n",
    "    dones = np.array([x[4] for x in minibatch])\n",
    "\n",
    "    states = states.reshape(states.shape[0], *self.observation_space)\n",
    "    next_states = next_states.reshape(next_states.shape[0], *self.observation_space)\n",
    "    dones = np.array([x[4] for x in minibatch])\n",
    "\n",
    "    targets = rewards + self.gamma * np.amax(self.model.predict_on_batch(next_states), axis=1) * (1 - dones)\n",
    "    target_f = self.model.predict_on_batch(states)\n",
    "\n",
    "    for i, action in enumerate(actions):\n",
    "        target_f[i][action] = targets[i]\n",
    "\n",
    "    self.model.fit(states, target_f, epochs=1, verbose=0)\n",
    "\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.205587500Z",
     "start_time": "2023-05-26T18:23:53.189935500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train(self, max_episode_length=500):\n",
    "    bar_format = 'Training: {percentage:3.0f}% |{bar}| Elapsed: {elapsed} Remaining: {remaining}{postfix}'\n",
    "    training_pbar = tqdm(total=self.episodes, bar_format=bar_format, unit='episode')\n",
    "    for e in range(self.episodes):\n",
    "        state = env.reset()\n",
    "        state = self.preprocess_frame(state)\n",
    "        state = np.reshape(state, [1, 84, 84, 1])\n",
    "        total_reward = 0\n",
    "        bar_format = 'Episode: {percentage:3.0f}% |{bar}| Speed: {rate_fmt}{postfix}'\n",
    "        episode_pbar = tqdm(total=max_episode_length, bar_format=bar_format, unit='step')\n",
    "        for time in range(max_episode_length):\n",
    "            action = self.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = self.preprocess_frame(next_state)\n",
    "            next_state = np.reshape(next_state, [1, 84, 84, 1])\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.episodes, time, self.epsilon))\n",
    "                break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay(self.batch_size)\n",
    "            total_reward += reward\n",
    "            episode_pbar.set_postfix_str(f'Reward: {total_reward}')\n",
    "            episode_pbar.update(1)\n",
    "        if e % 100 == 0:\n",
    "            self.model.save(self.model_name)\n",
    "        episode_pbar.close()\n",
    "        training_pbar.set_postfix_str(f'Reward: {total_reward}')\n",
    "        training_pbar.update(1)\n",
    "\n",
    "    training_pbar.close()\n",
    "    print('Training completed')\n",
    "    env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.219786400Z",
     "start_time": "2023-05-26T18:23:53.205587500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def play_with_model(model_path):\n",
    "    env = gym.make(GAME_NAME, render_mode='human')\n",
    "    saved_model = tf.keras.models.load_model(model_path)\n",
    "    agent = DQNAgent(INPUT_SHAPE, env.action_space.n)\n",
    "    agent.model = saved_model\n",
    "\n",
    "    state = env.reset()\n",
    "    state = agent.preprocess_frame(state)\n",
    "    state = np.reshape(state, [1, 84, 84, 1])\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if 'render_fps' in env.metadata:\n",
    "            env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = agent.preprocess_frame(next_state)\n",
    "        next_state = np.reshape(next_state, [1, 84, 84, 1])\n",
    "        state = next_state\n",
    "\n",
    "    env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.226122200Z",
     "start_time": "2023-05-26T18:23:53.219786400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0% |          | Elapsed: 00:00 Remaining: ?",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db763134392c4d9ba009ef7287570dcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'env'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 8\u001B[0m\n\u001B[0;32m      2\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(GAME_NAME)\n\u001B[0;32m      3\u001B[0m agent \u001B[38;5;241m=\u001B[39m DQNAgent(\n\u001B[0;32m      4\u001B[0m     model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrained_model.h5\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      5\u001B[0m     action_space\u001B[38;5;241m=\u001B[39menv\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mn,\n\u001B[0;32m      6\u001B[0m     episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m\n\u001B[0;32m      7\u001B[0m )\n\u001B[1;32m----> 8\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_episode_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<string>:5\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(self, max_episode_length)\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DQNAgent' object has no attribute 'env'"
     ]
    }
   ],
   "source": [
    "# if TRAIN:\n",
    "env = gym.make(GAME_NAME)\n",
    "agent = DQNAgent(\n",
    "    model_name='trained_model.h5',\n",
    "    action_space=env.action_space.n,\n",
    "    episodes=1000\n",
    ")\n",
    "agent.train(\n",
    "    max_episode_length=100\n",
    ")\n",
    "\n",
    "# play_with_model('trained_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:23:53.398000900Z",
     "start_time": "2023-05-26T18:23:53.226122200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: Keep in mind that observation space for this environment are frames from environment. Observation space is an image of size (210, 160, 3). so you will need to implement an agent which can process images!(a CNN based agent). \n",
    "\n",
    "Make sure to do perform preprocessing on the frames. For example, you can convert the RBG image to gray. you can use [OpenCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) library to perform resize\\ing, bluring or any applicable filtering on the frames."
   ],
   "metadata": {
    "id": "3CYO6rorZlrC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grading criteria\n",
    "Project: 35 points\n",
    "\n",
    "* Final Viva: 10 points\n",
    "* Implementation: 10 points\n",
    "* Final Report: 15 points\n",
    "\n",
    "For viva you will need to expilictly mention each team member's contribution.\n",
    "\n",
    "You can write your report on this notebook. The report must include visualization of your results. Train your model at least with 2 different sets of hyperparameters and in visualization section compare their output.\n"
   ],
   "metadata": {
    "id": "nUJd6RS3dot5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good Luck!"
   ],
   "metadata": {
    "id": "LjLQtO8UdtEj"
   }
  }
 ]
}
