{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "rl",
   "language": "python",
   "display_name": "RL"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL Final Project\n",
    "\n",
    "Now it's finally time to put into use what we have learned so far in this course!\n",
    "\n",
    "The aim of this project is to assess your practical knowledge in Reinforcement Learning.\n",
    "\n",
    "your project consist of 2 parts. you will get the chance to work with 2 different environment.\n"
   ],
   "metadata": {
    "id": "AQWaHq-KLtHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1.Resource Allocation\n",
    "\n",
    "First environment is a simplified simulation of a real-world problem called **Resource Allocation** more specifically computation resource allocation.  Computational resource allocation refers to the process of assigning and distributing computing resources, such as processing power, memory, and storage, to different tasks and applications. This allocation is typically managed by an operating system or a resource manager, which monitors the system's resource usage and ensures that each task or application receives the necessary resources to operate efficiently. The goal of computational resource allocation is to maximize the utilization of available resources while minimizing the impact on other tasks and ensuring that critical tasks are prioritized. Effective resource allocation is essential in optimizing the performance of a computing system and can have a significant impact on overall productivity and efficiency.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "ZgyVh_Tc4W0R"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Simulation:\n",
    "The architecture that we are using in this simulation is **[Edge Server](https://www.cloudflare.com/learning/cdn/glossary/edge-server/)** architecture. It consists of 3 entities in hierarchical manner.\n",
    "* The **Root** (first) layer is a ***cloud*** node with high computation resources.\n",
    "* The second layer is **Edge server** which maintains moderate compuation power (partially high comparing to enduser).\n",
    "* The last layer (access layer) is where **endpoints** are defined. this node has relatively low computational power. In this level workloads are generated.\n",
    "\n",
    "**Note**: Keep in mind that the communication is only [uplink](https://www.everythingrf.com/community/what-is-uplink) transmission is considered.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1G7UwG9bBQEYlisPSE21PZNvgx_Nm1fdj\"/>"
   ],
   "metadata": {
    "id": "47GkfZdTL6E6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The goal here is to minimize the computational delay for users.\n",
    "### 1.2 Action space : n + 2\n",
    "where n is the number of **edge** servers. \n",
    "\n",
    "Number ***2*** is representing two other possible action:\n",
    "  * Processing the workload **locally**\n",
    "  * Offloading the workload to **cload** server"
   ],
   "metadata": {
    "id": "DsWOuHarO5Ud"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your task is to train an agent using an algorithm of your choice."
   ],
   "metadata": {
    "id": "uHOZ5WQ3c11_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaXm730p4Roc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import networkx as nx\n",
    "from posixpath import expandvars\n",
    "\n",
    "\n",
    "\n",
    "class Env():\n",
    "    def __init__(self,endpoints, edgeservers,):\n",
    "        self.endpoints = endpoints\n",
    "        self.edgeservers = edgeservers\n",
    "        self.G = nx.Graph()\n",
    "        self.edges = []\n",
    "        self.nodes = np.zeros(1  + edgeservers + endpoints)  #this is the processing power\n",
    "        self.base_index = self.edgeservers + 1\n",
    "        self.resources = [np.zeros(1000)]+[np.zeros(r_edgeserver) for _ in range(self.edgeservers)] + [np.zeros(r_endpoint) for _ in range(self.endpoints)]\n",
    "        self.n_actions = 1 + self.edgeservers + 1    #  action[0] for local processing , action[1 : num_edgeservers] for processing in one of the edgeservers , action[-1] for processing in cloud\n",
    "        self.workloads = []\n",
    "        self.costs = np.zeros(self.endpoints)  # this array represents computation delay for enduser u. the bigger the delay, the bigger the cost.\n",
    "\n",
    "\n",
    "\n",
    "    def configure_network(self):\n",
    "\n",
    "        # declaring the edges between core cloud and the edgeserves\n",
    "        for j in range(1, self.edgeservers + 1):\n",
    "          # self.edges.append((0,j ))\n",
    "          self.G.add_edge(j,0)\n",
    "          self.G[j][0]['weight'] = random.uniform(400,500)\n",
    "\n",
    "\n",
    "        # declaring the edges between edgeservers\n",
    "        for i in range(1, self.edgeservers):\n",
    "          self.G.add_edge(i,i+1)\n",
    "          self.G.add_edge(i+1,i)\n",
    "          self.G[i][i+1]['weight'] = random.uniform(250,300)\n",
    "\n",
    "        # declaring the edges between edgeservers and endpoints\n",
    "        for i,j in zip(range(1 + self.edgeservers ,self.endpoints + 1 + self.edgeservers), np.resize(np.arange(1,self.edgeservers + 1), self.endpoints)):\n",
    "          self.G.add_edge(i,j)\n",
    "          self.G[i][j]['weight'] = random.uniform(180,250)\n",
    "\n",
    "        # print(self.G[0])\n",
    "\n",
    "        # declaring the processing power\n",
    "        self.nodes[0] = random.uniform(5000,7000) # Core cloud processing power\n",
    "        for i in range(1,self.base_index):\n",
    "          self.nodes[i] = random.uniform(500,2500) # Edge serverse processing power\n",
    "        for i in range(self.base_index, len(self.nodes)):\n",
    "          self.nodes[i] = random.uniform(50,300) # Endpoints porcessing power\n",
    "\n",
    "\n",
    "\n",
    "    def generate_task(self):\n",
    "        # generating workload values (fixed workloads) #first value is the offload and the second value is the emergency level\n",
    "        self.workloads = [(i, random.uniform(1e3 * 3.0, 1e4 * 3.0), np.random.choice([0,1], p=[0.7, 0.3]))  for i in range(self.endpoints)]\n",
    "        # return self.workloads\n",
    "\n",
    "    def clean_resources(self):\n",
    "        self.resources = [np.zeros(1000)]+[np.zeros(r_edgeserver) for _ in range(self.edgeservers)] + [np.zeros(r_endpoint) for _ in range(self.endpoints)]\n",
    "\n",
    "    def random_action(self):\n",
    "        self.actions =[random.randint(0,1 + self.edgeservers) for _ in range(self.endpoints)]\n",
    "        # return self.actions\n",
    "\n",
    "\n",
    "     ####################################################################################################################################   AUXILLARY FUNCTIONS\n",
    "     # The functions in this section\n",
    "     # are not mandatory to use. These are just\n",
    "     # auxillary functions that might accelerate\n",
    "     # your implementation\n",
    "\n",
    "    def initalize_Qtable(self):       #### it's not mandatory to use this function in your implementation\n",
    "        self.Qtable = np.zeros((self.endpoints, 1 + self.edgeservers + 1))\n",
    "        # print(self.Qtable)\n",
    "\n",
    "    def latency_calculator(self, node1, node2, W):        #### it's not mandatory to use this function in your implementation\n",
    "      latency = 0\n",
    "      if node2 == self.n_actions - 1 :\n",
    "        node2 = node1\n",
    "      path = nx.shortest_path(self.G, source = node1,  target = node2)\n",
    "      for i in range(len(path) - 1):\n",
    "        B = self.G[i][i+1]['weight']\n",
    "        latency += W / (B * self.network_gain())\n",
    "      latency += W / self.nodes[int(node2)]\n",
    "      return latency\n",
    "\n",
    "    def network_gain(self):         #### it's not mandatory to use this function in your implementation\n",
    "      return np.random.choice([0.5,0.25], p = [0.5,0.5])\n",
    "\n",
    "\n",
    "    def capacity_check(self, resource):       #### it's not mandatory to use this function in your implementation\n",
    "      c = False\n",
    "      for elem in resource:\n",
    "        if elem == 0:\n",
    "          c = True\n",
    "          break\n",
    "      return c\n",
    "\n",
    "    def allocate_task(self, resource, W, P):    #### it's not mandatory to use this function in your implementation\n",
    "      for i, slot in enumerate(resource):\n",
    "        if slot == 0:\n",
    "          resource[i] = W / P\n",
    "          return resource\n",
    "\n",
    "\n",
    "\n",
    "    def update_resource(self):      #### it's not mandatory to use this function in your implementation\n",
    "      for r in self.resources:\n",
    "        r -= 50\n",
    "        for i in range(len(r)):\n",
    "\n",
    "          if r[i] < 0:\n",
    "            r[i] = 0\n",
    "\n",
    "\n",
    "\n",
    "        #######################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def policy_action(self):\n",
    "      pass   #### Your implementation here\n",
    "\n",
    "\n",
    "    def update_values(self):\n",
    "      pass    #### Your implementation here\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "      pass    #### Your implementation here\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# hyper parameters\n",
    "K = 25\n",
    "r_endpoint  = 2 ### processing slots\n",
    "r_edgeserver = 5 ### processing slots\n",
    "discount_factor = 0.1\n",
    "learning_rate = 0.001\n",
    "epsilon = 0.99\n",
    "num_endusers = 10\n",
    "num_edgeservers = 3\n",
    "\n",
    "#### do not change this number\n",
    "time_slot_duration = 50 #seconds\n"
   ],
   "metadata": {
    "id": "v9-vPgF9Lzp6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def Algorithm(plot = False):\n",
    "\n",
    "  env = Env(num_endusers, num_edgeservers)\n",
    "  env.configure_network()\n",
    "\n",
    "\n",
    "  #### your implementation here"
   ],
   "metadata": {
    "id": "o4b-irU4L1GZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the results that I got from impelementing SARSA algorithm where *avg cost* is the average computational delay for all the users. \n",
    "$\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "$\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UIRkMEcb-9sAwOXDEpnuQV93WeFqJH-x\"/>"
   ],
   "metadata": {
    "id": "KYu4CMq9QZee"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Atari Game Pong"
   ],
   "metadata": {
    "id": "TdA4hr4kR33L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1FrWbdg-A30j7FAxT4zQacbeemCTFRPeh\"/>"
   ],
   "metadata": {
    "id": "bS8EasNeaVx-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[Pong](https://www.gymlibrary.dev/environments/atari/pong/)** is a famus atari game that almost all of us have played it at least once!\n",
    "The goal of this task is to get engage with **gym** library and use Deep Reinforcement Learning to train an agent which can actually play this game!"
   ],
   "metadata": {
    "id": "lD3mZJkBWGxp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n",
    "\n",
    "!pip install ALE gym\n",
    "!pip install \"gym[accept-rom-license, atari]\"\n",
    "!pip install opencv-python\n",
    "\n",
    "!pip install \"tensorflow<2.11\"\n",
    "!pip install tensorflow-gpu == 2.10\n",
    "\n",
    "!pip install tqdm\n",
    "!pip install jdc"
   ],
   "metadata": {
    "id": "nq9-gTd4Whko",
    "ExecuteTime": {
     "end_time": "2023-05-22T18:24:06.827116500Z",
     "start_time": "2023-05-22T18:24:04.422352400Z"
    }
   },
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jdc\n",
      "  Downloading jdc-0.0.9-py2.py3-none-any.whl (2.1 kB)\n",
      "Installing collected packages: jdc\n",
      "Successfully installed jdc-0.0.9\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports the necessary libraries and modules for the code, including Gym (for the RL environment), OpenCV (for image processing), NumPy (for numerical operations), TensorFlow (for deep learning), and Keras (for building and training the DQN model)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import jdc\n",
    "import warnings\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from IPython.utils import io\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "id": "p6T-o8XRQ54i",
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.679709100Z",
     "start_time": "2023-05-22T18:37:29.529668Z"
    }
   },
   "execution_count": 133,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Supress unimportant warnings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.690917600Z",
     "start_time": "2023-05-22T18:37:29.549745Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sets up the TensorFlow session to run on the GPU. It configures the session with the GPU options and sets it as the backend for Keras."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "with io.capture_output() as captured:\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.compat.v1.Session(config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(session)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.692917900Z",
     "start_time": "2023-05-22T18:37:29.569184400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defines the preprocess_frame method, which takes an observation from the environment and preprocesses the frame. It converts the frame to grayscale, resizes it to (84, 84) pixels, and normalizes the pixel values to the range [0, 1]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "def preprocess_frame(observation):\n",
    "    frame = observation[0]\n",
    "    if frame.ndim > 2:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = cv2.resize(frame, (84, 84))\n",
    "    frame = frame / 255.0\n",
    "    frame = np.expand_dims(frame, axis=-1)\n",
    "    return frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.693918400Z",
     "start_time": "2023-05-22T18:37:29.598254100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defines the DQNAgent class, which represents the DQN agent. It has methods to build the model, get an action, train the model, and run an episode. The model is built using Keras, consisting of convolutional layers, fully connected layers, and an output layer. The model is compiled with the Adam optimizer and mean squared error (MSE) loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, action_space, learning_rate):\n",
    "        self.input_shape = input_shape\n",
    "        self.action_space = action_space\n",
    "        self.model = self.build_model(learning_rate)\n",
    "\n",
    "    def build_model(self, alpha, activation='relu'):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(8, 8), strides=4, activation=activation, input_shape=self.input_shape))\n",
    "        model.add(Conv2D(64, kernel_size=(4, 4), strides=2, activation=activation))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), strides=1, activation=activation))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation=activation))\n",
    "        model.add(Dense(self.action_space))\n",
    "\n",
    "        optimizer = Adam(learning_rate=alpha)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.693918400Z",
     "start_time": "2023-05-22T18:37:29.608493400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defines the get_action method of the DQNAgent class. It takes a state and an epsilon value for epsilon-greedy exploration. With probability epsilon, it selects a random action. Otherwise, it uses the model to predict the Q-values for the given state and selects the action with the highest Q-value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "def get_action(self, state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.randint(self.action_space)\n",
    "    with io.capture_output() as captured:\n",
    "        q_values = self.model.predict(state)\n",
    "    return np.argmax(q_values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.694917600Z",
     "start_time": "2023-05-22T18:37:29.625277300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defines the train method of the DQNAgent class. It performs the training of the DQN agent for a specified number of episodes. It uses a progress bar to track the progress of the training. In each episode, it calls the run_episode method to run a single episode and update the model's weights. The epsilon value is decayed over episodes to gradually shift from exploration to exploitation. After training, the model is saved to a file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "def train(self, model_name, episodes, epsilon_decay, epsilon_start=1.0, epsilon_end=0.1, gamma=0.97,max_episode_length=100):\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    bar_format = 'Training: {percentage:3.0f}% |{bar}| Elapsed: {elapsed}, Remaining: {remaining}{postfix}'\n",
    "    training_pbar = tqdm(total=episodes, bar_format=bar_format, unit='episode')\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        total_reward = self.run_episode(epsilon, gamma, max_episode_length)\n",
    "        training_pbar.set_postfix_str(f'Reward: {total_reward}')\n",
    "        training_pbar.update(1)\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    training_pbar.close()\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    self.model.save(model_name)\n",
    "    print(\"Model saved.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.695919900Z",
     "start_time": "2023-05-22T18:37:29.642620400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defines the run_episode method of the DQNAgent class. It runs a single episode of the environment using the current policy and updates the model's weights. It iteratively selects actions, observes the next state and reward, and performs a model update using the Q-learning algorithm. The progress is tracked using a separate progress bar."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "def run_episode(self, epsilon, gamma, max_episode_length):\n",
    "        observation = env.reset()\n",
    "        state = preprocess_frame(observation)\n",
    "        state = np.reshape(state, (1, *self.input_shape))\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        bar_format = 'Episode: {percentage:3.0f}% |{bar}| Speed: {rate_fmt}{postfix}'\n",
    "        episode_pbar = tqdm(total=max_episode_length, bar_format=bar_format, unit='step')\n",
    "\n",
    "        while not done:\n",
    "            action = self.get_action(state, epsilon)\n",
    "            next_observation, reward, done, _, _ = env.step(action)\n",
    "            next_state = preprocess_frame(next_observation)\n",
    "            if next_state is not None:\n",
    "                next_state = np.reshape(next_state, (1, *self.input_shape))\n",
    "                with io.capture_output() as captured:\n",
    "                    value = np.array([reward + gamma * np.max(self.model.predict(next_state))])\n",
    "                    self.model.fit(state, value, verbose=0)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                episode_length += 1\n",
    "                episode_pbar.set_postfix_str(f'Reward: {total_reward}')\n",
    "                episode_pbar.update(1)\n",
    "\n",
    "            if episode_length >= max_episode_length:\n",
    "                done = True\n",
    "\n",
    "        episode_pbar.close()\n",
    "        return total_reward"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.706918100Z",
     "start_time": "2023-05-22T18:37:29.658103700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sets up the Pong environment using Gym. It creates an instance of the environment and resets it to obtain the initial observation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "# env = gym.make(\"ALE/Pong-v5\", render_mode='human')\n",
    "env = gym.make(\"ALE/Pong-v5\")\n",
    "observation = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.789784Z",
     "start_time": "2023-05-22T18:37:29.759294700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creates an instance of the DQNAgent class. It specifies the input shape, action space size, and learning rate for the agent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "    input_shape=(84, 84, 1),\n",
    "    action_space=env.action_space.n,\n",
    "    learning_rate=0.00025\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T18:37:29.858845500Z",
     "start_time": "2023-05-22T18:37:29.798298300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trains the agent by calling the train method. It specifies the model name for saving, the number of episodes, epsilon decay rate, and maximum episode length. After training, it closes the environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0% |          | Elapsed: 00:00, Remaining: ?",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92fbb154fa7849368e0a00e7de4f9119"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Episode:   0% |          | Speed: ?step/s",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec24ceff52744ee7adf7b3ebed10d068"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Episode:   0% |          | Speed: ?step/s",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "085d21185ee746a5b0272e4d8d57516b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.train(\n",
    "    model_name='trained_model.h5',\n",
    "    episodes=50,\n",
    "    epsilon_decay=0.99,\n",
    "    max_episode_length=500\n",
    ")\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-22T18:37:29.863353400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: Keep in mind that observation space for this environment are frames from environment. Observation space is an image of size (210, 160, 3). so you will need to implement an agent which can process images!(a CNN based agent). \n",
    "\n",
    "Make sure to do perform preprocessing on the frames. For example, you can convert the RBG image to gray. you can use [OpenCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) library to perform resize\\ing, bluring or any applicable filtering on the frames."
   ],
   "metadata": {
    "id": "3CYO6rorZlrC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grading criteria\n",
    "Project: 35 points\n",
    "\n",
    "* Final Viva: 10 points\n",
    "* Implementation: 10 points\n",
    "* Final Report: 15 points\n",
    "\n",
    "For viva you will need to expilictly mention each team member's contribution.\n",
    "\n",
    "You can write your report on this notebook. The report must include visualization of your results. Train your model at least with 2 different sets of hyperparameters and in visualization section compare their output.\n"
   ],
   "metadata": {
    "id": "nUJd6RS3dot5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good Luck!"
   ],
   "metadata": {
    "id": "LjLQtO8UdtEj"
   }
  }
 ]
}
