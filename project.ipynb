{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL Final Project\n",
    "\n",
    "Now it's finally time to put into use what we have learned so far in this course!\n",
    "\n",
    "The aim of this project is to assess your practical knowledge in Reinforcement Learning.\n",
    "\n",
    "your project consist of 2 parts. you will get the chance to work with 2 different environment.\n"
   ],
   "metadata": {
    "id": "AQWaHq-KLtHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1.Resource Allocation\n",
    "\n",
    "First environment is a simplified simulation of a real-world problem called **Resource Allocation** more specifically computation resource allocation.  Computational resource allocation refers to the process of assigning and distributing computing resources, such as processing power, memory, and storage, to different tasks and applications. This allocation is typically managed by an operating system or a resource manager, which monitors the system's resource usage and ensures that each task or application receives the necessary resources to operate efficiently. The goal of computational resource allocation is to maximize the utilization of available resources while minimizing the impact on other tasks and ensuring that critical tasks are prioritized. Effective resource allocation is essential in optimizing the performance of a computing system and can have a significant impact on overall productivity and efficiency.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "ZgyVh_Tc4W0R"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Simulation:\n",
    "The architecture that we are using in this simulation is **[Edge Server](https://www.cloudflare.com/learning/cdn/glossary/edge-server/)** architecture. It consists of 3 entities in hierarchical manner.\n",
    "* The **Root** (first) layer is a ***cloud*** node with high computation resources.\n",
    "* The second layer is **Edge server** which maintains moderate compuation power (partially high comparing to enduser).\n",
    "* The last layer (access layer) is where **endpoints** are defined. this node has relatively low computational power. In this level workloads are generated.\n",
    "\n",
    "**Note**: Keep in mind that the communication is only [uplink](https://www.everythingrf.com/community/what-is-uplink) transmission is considered.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1G7UwG9bBQEYlisPSE21PZNvgx_Nm1fdj\"/>"
   ],
   "metadata": {
    "id": "47GkfZdTL6E6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* The goal here is to minimize the computational delay for users.\n",
    "### 1.2 Action space : n + 2\n",
    "where n is the number of **edge** servers. \n",
    "\n",
    "Number ***2*** is representing two other possible action:\n",
    "  * Processing the workload **locally**\n",
    "  * Offloading the workload to **cload** server"
   ],
   "metadata": {
    "id": "DsWOuHarO5Ud"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your task is to train an agent using an algorithm of your choice."
   ],
   "metadata": {
    "id": "uHOZ5WQ3c11_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaXm730p4Roc"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "# import itertools\n",
    "# import scipy.misc\n",
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# import networkx as nx\n",
    "# from posixpath import expandvars\n",
    "#\n",
    "#\n",
    "#\n",
    "# class Env():\n",
    "#     def __init__(self,endpoints, edgeservers,):\n",
    "#         self.endpoints = endpoints\n",
    "#         self.edgeservers = edgeservers\n",
    "#         self.G = nx.Graph()\n",
    "#         self.edges = []\n",
    "#         self.nodes = np.zeros(1  + edgeservers + endpoints)  #this is the processing power\n",
    "#         self.base_index = self.edgeservers + 1\n",
    "#         self.resources = [np.zeros(1000)]+[np.zeros(r_edgeserver) for _ in range(self.edgeservers)] + [np.zeros(r_endpoint) for _ in range(self.endpoints)]\n",
    "#         self.n_actions = 1 + self.edgeservers + 1    #  action[0] for local processing , action[1 : num_edgeservers] for processing in one of the edgeservers , action[-1] for processing in cloud\n",
    "#         self.workloads = []\n",
    "#         self.costs = np.zeros(self.endpoints)  # this array represents computation delay for enduser u. the bigger the delay, the bigger the cost.\n",
    "#\n",
    "#\n",
    "#\n",
    "#     def configure_network(self):\n",
    "#\n",
    "#         # declaring the edges between core cloud and the edgeserves\n",
    "#         for j in range(1, self.edgeservers + 1):\n",
    "#           # self.edges.append((0,j ))\n",
    "#           self.G.add_edge(j,0)\n",
    "#           self.G[j][0]['weight'] = random.uniform(400,500)\n",
    "#\n",
    "#\n",
    "#         # declaring the edges between edgeservers\n",
    "#         for i in range(1, self.edgeservers):\n",
    "#           self.G.add_edge(i,i+1)\n",
    "#           self.G.add_edge(i+1,i)\n",
    "#           self.G[i][i+1]['weight'] = random.uniform(250,300)\n",
    "#\n",
    "#         # declaring the edges between edgeservers and endpoints\n",
    "#         for i,j in zip(range(1 + self.edgeservers ,self.endpoints + 1 + self.edgeservers), np.resize(np.arange(1,self.edgeservers + 1), self.endpoints)):\n",
    "#           self.G.add_edge(i,j)\n",
    "#           self.G[i][j]['weight'] = random.uniform(180,250)\n",
    "#\n",
    "#         # print(self.G[0])\n",
    "#\n",
    "#         # declaring the processing power\n",
    "#         self.nodes[0] = random.uniform(5000,7000) # Core cloud processing power\n",
    "#         for i in range(1,self.base_index):\n",
    "#           self.nodes[i] = random.uniform(500,2500) # Edge serverse processing power\n",
    "#         for i in range(self.base_index, len(self.nodes)):\n",
    "#           self.nodes[i] = random.uniform(50,300) # Endpoints porcessing power\n",
    "#\n",
    "#\n",
    "#\n",
    "#     def generate_task(self):\n",
    "#         # generating workload values (fixed workloads) #first value is the offload and the second value is the emergency level\n",
    "#         self.workloads = [(i, random.uniform(1e3 * 3.0, 1e4 * 3.0), np.random.choice([0,1], p=[0.7, 0.3]))  for i in range(self.endpoints)]\n",
    "#         # return self.workloads\n",
    "#\n",
    "#     def clean_resources(self):\n",
    "#         self.resources = [np.zeros(1000)]+[np.zeros(r_edgeserver) for _ in range(self.edgeservers)] + [np.zeros(r_endpoint) for _ in range(self.endpoints)]\n",
    "#\n",
    "#     def random_action(self):\n",
    "#         self.actions =[random.randint(0,1 + self.edgeservers) for _ in range(self.endpoints)]\n",
    "#         # return self.actions\n",
    "#\n",
    "#\n",
    "#      ####################################################################################################################################   AUXILLARY FUNCTIONS\n",
    "#      # The functions in this section\n",
    "#      # are not mandatory to use. These are just\n",
    "#      # auxillary functions that might accelerate\n",
    "#      # your implementation\n",
    "#\n",
    "#     def initalize_Qtable(self):       #### it's not mandatory to use this function in your implementation\n",
    "#         self.Qtable = np.zeros((self.endpoints, 1 + self.edgeservers + 1))\n",
    "#         # print(self.Qtable)\n",
    "#\n",
    "#     def latency_calculator(self, node1, node2, W):        #### it's not mandatory to use this function in your implementation\n",
    "#       latency = 0\n",
    "#       if node2 == self.n_actions - 1 :\n",
    "#         node2 = node1\n",
    "#       path = nx.shortest_path(self.G, source = node1,  target = node2)\n",
    "#       for i in range(len(path) - 1):\n",
    "#         B = self.G[i][i+1]['weight']\n",
    "#         latency += W / (B * self.network_gain())\n",
    "#       latency += W / self.nodes[int(node2)]\n",
    "#       return latency\n",
    "#\n",
    "#     def network_gain(self):         #### it's not mandatory to use this function in your implementation\n",
    "#       return np.random.choice([0.5,0.25], p = [0.5,0.5])\n",
    "#\n",
    "#\n",
    "#     def capacity_check(self, resource):       #### it's not mandatory to use this function in your implementation\n",
    "#       c = False\n",
    "#       for elem in resource:\n",
    "#         if elem == 0:\n",
    "#           c = True\n",
    "#           break\n",
    "#       return c\n",
    "#\n",
    "#     def allocate_task(self, resource, W, P):    #### it's not mandatory to use this function in your implementation\n",
    "#       for i, slot in enumerate(resource):\n",
    "#         if slot == 0:\n",
    "#           resource[i] = W / P\n",
    "#           return resource\n",
    "#\n",
    "#\n",
    "#\n",
    "#     def update_resource(self):      #### it's not mandatory to use this function in your implementation\n",
    "#       for r in self.resources:\n",
    "#         r -= 50\n",
    "#         for i in range(len(r)):\n",
    "#\n",
    "#           if r[i] < 0:\n",
    "#             r[i] = 0\n",
    "#\n",
    "#\n",
    "#\n",
    "#         #######################################################################################################################################\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#     def policy_action(self):\n",
    "#       pass   #### Your implementation here\n",
    "#\n",
    "#\n",
    "#     def update_values(self):\n",
    "#       pass    #### Your implementation here\n",
    "#\n",
    "#\n",
    "#     def step(self):\n",
    "#       pass    #### Your implementation here\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# # hyperparameters\n",
    "# K = 25\n",
    "# r_endpoint  = 2 ### processing slots\n",
    "# r_edgeserver = 5 ### processing slots\n",
    "# discount_factor = 0.1\n",
    "# learning_rate = 0.001\n",
    "# epsilon = 0.99\n",
    "# num_endusers = 10\n",
    "# num_edgeservers = 3\n",
    "#\n",
    "# #### do not change this number\n",
    "# time_slot_duration = 50 #seconds\n",
    "#"
   ],
   "metadata": {
    "id": "v9-vPgF9Lzp6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# def Algorithm(plot = False):\n",
    "#\n",
    "#   env = Env(num_endusers, num_edgeservers)\n",
    "#   env.configure_network()\n",
    "\n",
    "\n",
    "#### your implementation here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "o4b-irU4L1GZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the results that I got from impelementing SARSA algorithm where *avg cost* is the average computational delay for all the users. \n",
    "$\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\n",
    "$\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UIRkMEcb-9sAwOXDEpnuQV93WeFqJH-x\"/>"
   ],
   "metadata": {
    "id": "KYu4CMq9QZee"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Atari Game Pong"
   ],
   "metadata": {
    "id": "TdA4hr4kR33L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1FrWbdg-A30j7FAxT4zQacbeemCTFRPeh\"/>"
   ],
   "metadata": {
    "id": "bS8EasNeaVx-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[Pong](https://www.gymlibrary.dev/environments/atari/pong/)** is a famus atari game that almost all of us have played it at least once!\n",
    "The goal of this task is to get engage with **gym** library and use Deep Reinforcement Learning to train an agent which can actually play this game!"
   ],
   "metadata": {
    "id": "lD3mZJkBWGxp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install ALE gym\n",
    "!pip install \"gym[accept-rom-license, atari]\"\n",
    "!pip install opencv-python\n",
    "!pip install tensorflow keras"
   ],
   "metadata": {
    "id": "nq9-gTd4Whko"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import necessary modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.optimizers import Adam"
   ],
   "metadata": {
    "id": "p6T-o8XRQ54i"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, action_space, learning_rate):\n",
    "        self.input_shape = input_shape\n",
    "        self.action_space = action_space\n",
    "        self.model = self.build_model(learning_rate)\n",
    "\n",
    "    def build_model(self, learning_rate):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(8, 8), strides=(4, 4), activation='relu', input_shape=self.input_shape))\n",
    "        model.add(Conv2D(64, kernel_size=(4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        return model\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        if frame.ndim == 3:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        elif frame.ndim == 2:\n",
    "            frame = frame  # Frame is already grayscale, no conversion needed\n",
    "\n",
    "        frame = cv2.resize(frame, (84, 84))\n",
    "        frame = frame / 255.0\n",
    "        return frame\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return np.random.randint(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def train(self, episodes, epsilon_start, epsilon_end, epsilon_decay):\n",
    "        epsilon = epsilon_start\n",
    "        pbar = tqdm(total=episodes, initial=0, desc=\"Training\", unit=\"episode\")\n",
    "\n",
    "        for episode in range(1, episodes):\n",
    "            try:\n",
    "                observation = env.reset()\n",
    "                frame = observation[0]\n",
    "                state = self.preprocess_frame(frame)\n",
    "                state = np.reshape(state, (1, 84, 84, 1))\n",
    "                done = False\n",
    "                total_reward = 0\n",
    "                while not done:\n",
    "                    if 'render_fps' in env.metadata:\n",
    "                        env.render()\n",
    "                    action = self.get_action(state, epsilon)\n",
    "                    next_observation, reward, done, truncated, info = env.step(action)\n",
    "                    next_frame = next_observation[0]\n",
    "                    next_state = self.preprocess_frame(next_frame)\n",
    "\n",
    "                    if next_state is not None:\n",
    "                        next_state = np.reshape(next_state, (1, 84, 84, 1))\n",
    "                        self.model.fit(state, np.array([reward + 0.99 * np.max(self.model.predict(next_state))]),\n",
    "                                       verbose=0)\n",
    "                        state = next_state\n",
    "                        total_reward += reward\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "            pbar.set_postfix({\"Total Reward\": total_reward, \"Epsilon\": epsilon})\n",
    "            pbar.update(1)\n",
    "\n",
    "            epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        pbar.close()\n",
    "        env.close()\n",
    "        print(\"Training completed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T15:41:37.825267600Z",
     "start_time": "2023-05-21T15:41:37.814976200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [00:00<?, ?episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [00:13<?, ?episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", render_mode='human')\n",
    "\n",
    "input_shape = (84, 84, 1)\n",
    "action_space = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(\n",
    "    input_shape,\n",
    "    action_space,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "agent.train(\n",
    "    episodes=100,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.1,\n",
    "    epsilon_decay=0.99\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T15:41:54.325326300Z",
     "start_time": "2023-05-21T15:41:40.664376900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: Keep in mind that observation space for this environment are frames from environment. Observation space is an image of size (210, 160, 3). so you will need to implement an agent which can process images!(a CNN based agent). \n",
    "\n",
    "Make sure to do perform preprocessing on the frames. For example, you can convert the RBG image to gray. you can use [OpenCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) library to perform resize\\ing, bluring or any applicable filtering on the frames."
   ],
   "metadata": {
    "id": "3CYO6rorZlrC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grading criteria\n",
    "Project: 35 points\n",
    "\n",
    "* Final Viva: 10 points\n",
    "* Implementation: 10 points\n",
    "* Final Report: 15 points\n",
    "\n",
    "For viva you will need to expilictly mention each team member's contribution.\n",
    "\n",
    "You can write your report on this notebook. The report must include visualization of your results. Train your model at least with 2 different sets of hyperparameters and in visualization section compare their output.\n"
   ],
   "metadata": {
    "id": "nUJd6RS3dot5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good Luck!"
   ],
   "metadata": {
    "id": "LjLQtO8UdtEj"
   }
  }
 ]
}
