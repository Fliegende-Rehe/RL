{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL Final Project\n",
    "\n",
    "Now it's finally time to put into use what we have learned so far in this course!\n",
    "\n",
    "The aim of this project is to assess your practical knowledge in Reinforcement Learning.\n",
    "\n",
    "your project consist of 2 parts. you will get the chance to work with 2 different environment.\n"
   ],
   "metadata": {
    "id": "AQWaHq-KLtHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Atari Game Pong"
   ],
   "metadata": {
    "id": "TdA4hr4kR33L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "bS8EasNeaVx-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[Pong](https://www.gymlibrary.dev/environments/atari/pong/)** is a famus atari game that almost all of us have played it at least once!\n",
    "The goal of this task is to get engage with **gym** library and use Deep Reinforcement Learning to train an agent which can actually play this game!"
   ],
   "metadata": {
    "id": "lD3mZJkBWGxp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install ALE gym\n",
    "!pip install \"gym[accept-rom-license, atari]\""
   ],
   "metadata": {
    "id": "nq9-gTd4Whko"
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[accept-rom-license,atari] in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[accept-rom-license,atari]) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[accept-rom-license,atari]) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[accept-rom-license,atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (5.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.26.0)\n",
      "Requirement already satisfied: click in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.65.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2021.5.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You might face some problems rendering the environments in colab. so for this task try running it locally in your machine."
   ],
   "metadata": {
    "id": "_SbNcuaZZDo9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "class LearningConstants:\n",
    "    discount_factor: float = 0.99\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    width = 30\n",
    "    height = 30\n",
    "    policy_learning_rate = 0.01\n",
    "    state_value_approximation_learning_rate = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms.functional import crop, to_grayscale, adjust_contrast, resize\n",
    "class ProcessState:\n",
    "\n",
    "    def __init__(self, tensor_state: torch.Tensor) -> None:\n",
    "        self.tensor_state: torch.Tensor = tensor_state\n",
    "\n",
    "    @staticmethod\n",
    "    def fromObservation(observation: numpy.ndarray):\n",
    "\n",
    "        observation = Image.fromarray(observation)\n",
    "        observation = crop(observation, top=34, left=0, height=160, width=160)\n",
    "        observation = resize(observation, [LearningConstants.height, LearningConstants.width])\n",
    "        observation = adjust_contrast(to_grayscale(observation), 100.0)\n",
    "\n",
    "        # observation.show()\n",
    "\n",
    "        return ProcessState(\n",
    "            tensor_state = torch.tensor(numpy.asarray(observation))\n",
    "        )\n",
    "\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> torch.Tensor:\n",
    "        return self.tensor_state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "class Step:\n",
    "\n",
    "    def __init__(self, observation_tensor, action, reward, log_prob) -> None:\n",
    "        self.observation_tensor = observation_tensor\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.log_prob = log_prob\n",
    "\n",
    "\n",
    "class History:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.steps = []\n",
    "\n",
    "    def addStep(self, step: Step) -> None:\n",
    "        self.steps.append(step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.functional import sigmoid\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, shape, n_actions) -> None:\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=2, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out((30, 30))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return sigmoid(self.fc(conv_out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StateValueApproximationNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, shape) -> None:\n",
    "        super(StateValueApproximationNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=2, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out((30, 30))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            policy_network: PolicyNetwork,\n",
    "            state_value_approximation_network: StateValueApproximationNetwork) -> None:\n",
    "        self.policy_network: PolicyNetwork = policy_network\n",
    "        self.state_value_approximation_network: StateValueApproximationNetwork = state_value_approximation_network\n",
    "\n",
    "        self.policy_network_optimizer = torch.optim.Adam(self.policy_network.parameters(), lr = LearningConstants.policy_learning_rate)\n",
    "        self.state_value_approximation_network_optimizer = torch.optim.Adam(self.state_value_approximation_network.parameters(),\n",
    "                                                                            lr = LearningConstants.state_value_approximation_learning_rate)\n",
    "        self.previous_state: ProcessState = None\n",
    "\n",
    "    def selectAction(self, state: torch.Tensor) -> tuple:\n",
    "        state_tensor: torch.Tensor = state.float().unsqueeze(0).to(LearningConstants.device)\n",
    "        # print(state_tensor)\n",
    "        # print(state_tensor.shape)\n",
    "        probability_of_actions = self.policy_network(state_tensor)\n",
    "        distr = torch.distributions.Categorical(probability_of_actions[0])\n",
    "        action = distr.sample()\n",
    "        return action.item(), distr.log_prob(action)\n",
    "\n",
    "    def convertRewardsToCummulative(self, rewards_array: list) -> torch.Tensor:\n",
    "        # CDR - Cummulative discounted rewards\n",
    "        CDR = list()\n",
    "        cummulation = 0\n",
    "        for reward in rewards_array[::-1]:\n",
    "            cummulation = reward + cummulation * LearningConstants.discount_factor\n",
    "            CDR.append(cummulation)\n",
    "        CDR.reverse()\n",
    "        CDR = torch.tensor(CDR).to(LearningConstants.device)\n",
    "        return ((CDR - CDR.mean()) / CDR.std())\n",
    "\n",
    "    def trainPolicyNetwork(self, deltas, log_probs) -> float:\n",
    "        policy_loss = []\n",
    "        for i in range(len(deltas)):\n",
    "            policy_loss.append(\n",
    "                deltas[i] * log_probs[i] * -1\n",
    "            )\n",
    "        self.policy_network_optimizer.zero_grad()\n",
    "        sum(policy_loss).backward()\n",
    "        self.policy_network_optimizer.step()\n",
    "        return sum(policy_loss).item()\n",
    "\n",
    "    def trainStateValueApproximationNetwork(self, CDR, SPV) -> float:\n",
    "        #CDR - cummulative discounted reward\n",
    "        #SPV - state predicted values\n",
    "        loss = torch.nn.functional.mse_loss(SPV, CDR)\n",
    "        self.state_value_approximation_network_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.state_value_approximation_network_optimizer.step()\n",
    "        return loss.sum().item()\n",
    "\n",
    "    def setPreviousState(self, state: ProcessState) -> None:\n",
    "        self.previous_state = state\n",
    "\n",
    "    def getStateChangeAndUpdatePrevious(self, current_state: ProcessState) -> torch.Tensor:\n",
    "        tensor_to_return = current_state.tensor_state - self.previous_state.tensor_state if self.previous_state is not None else current_state.tensor_state\n",
    "        self.previous_state = current_state\n",
    "        return tensor_to_return\n",
    "\n",
    "    def learn(self, history: History) -> tuple[float, float]:\n",
    "        states = [step.observation_tensor for step in history.steps]\n",
    "        actions = [step.action for step in history.steps]\n",
    "        rewards = [step.reward for step in history.steps]\n",
    "        log_probs = [step.log_prob for step in history.steps]\n",
    "\n",
    "        CDR = self.convertRewardsToCummulative(rewards)\n",
    "        state_values = []\n",
    "        for state in states:\n",
    "            state = state.float().unsqueeze(0).to(LearningConstants.device)\n",
    "            state_values.append(self.state_value_approximation_network(state))\n",
    "        state_values = torch.stack(state_values).squeeze()\n",
    "        loss_SV_ANN = self.trainStateValueApproximationNetwork(CDR, state_values)\n",
    "        deltas = [gt - val for gt, val in zip(CDR, state_values)]\n",
    "        deltas = torch.tensor(deltas).to(LearningConstants.device)\n",
    "        loss_PLC_ANN = self.trainPolicyNetwork(deltas, log_probs)\n",
    "        return loss_SV_ANN, loss_PLC_ANN\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "p6T-o8XRQ54i"
   },
   "execution_count": 119,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "\n",
    "agent = Agent(\n",
    "    PolicyNetwork(\n",
    "        (LearningConstants.width, LearningConstants.height), 6\n",
    "    ),\n",
    "    StateValueApproximationNetwork(\n",
    "        (LearningConstants.width, LearningConstants.height)\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 84/1000 [23:26<4:15:35, 16.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6116/3635604560.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m             \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m     \u001B[0mscores_reached\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscore\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m     \u001B[0mSV_ANN_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mPLC_ANN_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlearn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'results_2.txt'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'a'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfile\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m         \u001B[0mfile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{score};{SV_ANN_loss};{PLC_ANN_loss}\\n\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6116/2992873377.py\u001B[0m in \u001B[0;36mlearn\u001B[1;34m(self, history)\u001B[0m\n\u001B[0;32m     70\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mstate\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstates\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m             \u001B[0mstate\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mLearningConstants\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m             \u001B[0mstate_values\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_value_approximation_network\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     73\u001B[0m         \u001B[0mstate_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_values\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m         \u001B[0mloss_SV_ANN\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrainStateValueApproximationNetwork\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mCDR\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate_values\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1499\u001B[0m                 \u001B[1;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1500\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1502\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1503\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6116/2179666685.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[0mconv_out\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconv_out\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1499\u001B[0m                 \u001B[1;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1500\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1502\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1503\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    216\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 217\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    218\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    219\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1499\u001B[0m                 \u001B[1;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1500\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1502\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1503\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    102\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 103\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minplace\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    104\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    105\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\bulkin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mrelu\u001B[1;34m(input, inplace)\u001B[0m\n\u001B[0;32m   1455\u001B[0m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1456\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1457\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1458\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1459\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "scores_reached = []\n",
    "\n",
    "import gym\n",
    "with open('results_2.txt', 'w') as file:\n",
    "    file.write('\\n')\n",
    "\n",
    "\n",
    "max_score = -21\n",
    "env = gym.make(\"ALE/Pong-v5\") # create the Pong environment\n",
    "epsilon = 0.5 - ((max_score + 21) / 42) * 0.5\n",
    "\n",
    "for episode in tqdm(range(1000)):\n",
    "    observation, _ = env.reset() # reset the environment and get the initial observation\n",
    "    state_change = agent.getStateChangeAndUpdatePrevious(\n",
    "        ProcessState.fromObservation(observation)\n",
    "    )\n",
    "    history = History()\n",
    "    score = 0\n",
    "    while True:\n",
    "        # print(state_change)\\\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        action, log_prob = agent.selectAction(state_change)\n",
    "        new_observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        new_state = ProcessState.fromObservation(new_observation)\n",
    "        state_change = agent.getStateChangeAndUpdatePrevious(new_state)\n",
    "        score += reward\n",
    "        history.addStep(Step(state_change, action, reward, log_prob))\n",
    "        if done:\n",
    "            break\n",
    "    scores_reached.append(score)\n",
    "    SV_ANN_loss, PLC_ANN_loss = agent.learn(history)\n",
    "    with open('results_2.txt', 'a') as file:\n",
    "        file.write(f\"{score};{SV_ANN_loss};{PLC_ANN_loss}\\n\")\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        torch.save(agent.state_value_approximation_network, \"SVA_ANN.ann\")\n",
    "        torch.save(agent.policy_network, \"PLC_ANN.ann\")\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "critic_loss_1: list[float] = list()\n",
    "critic_loss_2: list[float] = list()\n",
    "with open('results_1.txt', 'r') as file:\n",
    "    for line in file.read().split('\\n'):\n",
    "        try: critic_loss_1.append(float(line.split(';')[1]))\n",
    "        except Exception: continue\n",
    "with open('results_2.txt', 'r') as file:\n",
    "    for line in file.read().split('\\n'):\n",
    "        try: critic_loss_2.append(float(line.split(';')[1]))\n",
    "        except Exception: continue\n",
    "\n",
    "plt.plot(critic_loss_1[3:], color='red', label='DF = 0.6')\n",
    "plt.plot(critic_loss_2[3:], color='blue', label ='DF = 0.99')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good Luck!"
   ],
   "metadata": {
    "id": "LjLQtO8UdtEj"
   }
  }
 ]
}