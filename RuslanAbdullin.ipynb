{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "rl",
   "language": "python",
   "display_name": "RL"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL Final Project\n",
    "\n",
    "Now it's finally time to put into use what we have learned so far in this course!\n",
    "\n",
    "The aim of this project is to assess your practical knowledge in Reinforcement Learning.\n",
    "\n",
    "your project consist of 2 parts. you will get the chance to work with 2 different environment.\n"
   ],
   "metadata": {
    "id": "AQWaHq-KLtHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Atari Game Pong"
   ],
   "metadata": {
    "id": "TdA4hr4kR33L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"zzzzzzzzzzzzzzzzzc\"/>"
   ],
   "metadata": {
    "id": "bS8EasNeaVx-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[Pong](https://www.gymlibrary.dev/environments/atari/pong/)** is a famus atari game that almost all of us have played it at least once!\n",
    "The goal of this task is to get engage with **gym** library and use Deep Reinforcement Learning to train an agent which can actually play this game!"
   ],
   "metadata": {
    "id": "lD3mZJkBWGxp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install ALE\n",
    "# !pip install gym\n",
    "# !pip install opencv-python\n",
    "#\n",
    "# !pip install \"tensorflow==2.10\"\n",
    "# !pip install \"tensorflow-gpu==2.10\"\n",
    "#\n",
    "# !pip install tqdm\n",
    "# !pip install jdc\n",
    "#\n",
    "# !pip list"
   ],
   "metadata": {
    "id": "nq9-gTd4Whko",
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:14.950758Z",
     "start_time": "2023-05-26T16:37:14.828503400Z"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports the necessary libraries and modules for the code, including Gym (for the RL environment), OpenCV (for image processing), NumPy (for numerical operations), TensorFlow (for deep learning), and Keras (for building and training the DQN model)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import cv2\n",
    "import jdc\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.utils import io\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "id": "p6T-o8XRQ54i",
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:14.950758Z",
     "start_time": "2023-05-26T16:37:14.878825700Z"
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Supress unimportant warnings\n",
    "Define constant\n",
    "Check if tensorflow found any GPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "INPUT_SHAPE = (84, 84, 1)\n",
    "TRAIN = False\n",
    "GAME_NAME = 'ALE/Pong-v5'\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:14.978279100Z",
     "start_time": "2023-05-26T16:37:14.926642Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defines the DQNAgent class, which represents the DQN (Deep Q-Network) agent. The DQNAgent has methods to build the model, get an action, store experiences, replay experiences for training, and run episodes. The model is built using the Keras Sequential API and consists of convolutional layers, fully connected layers, and an output layer. The model is compiled with the Adam optimizer and mean squared error (MSE) loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, learning_rate=0.0001, batch_size=32, memory_size=20000, update_frequency=1000):\n",
    "        self.env = gym.make(GAME_NAME)  # , render_mode='human')\n",
    "        self.action_space = self.env.action_space.n\n",
    "        self.model = self.build_model(learning_rate)\n",
    "        self.target_model = self.build_model(learning_rate)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.memory_size = memory_size\n",
    "        self.update_frequency = update_frequency\n",
    "        self.steps = 0\n",
    "\n",
    "    def build_model(self, learning_rate, activation='relu'):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Conv2D(32, kernel_size=(8, 8), strides=(4, 4), activation=activation,\n",
    "                                         input_shape=INPUT_SHAPE))\n",
    "        model.add(tf.keras.layers.Conv2D(64, kernel_size=(4, 4), strides=(2, 2), activation=activation))\n",
    "        model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation=activation))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(512, activation=activation))\n",
    "        model.add(tf.keras.layers.Dense(self.action_space))\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:15.038946Z",
     "start_time": "2023-05-26T16:37:15.008947900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method get_action to the DQNAgent class. It takes a state and an epsilon value for epsilon-greedy exploration. With probability epsilon, it selects a random action. Otherwise, it uses the model to predict the Q-values for the given state and selects the action with the highest Q-value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def get_action(self, state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.randint(self.action_space)\n",
    "    with io.capture_output() as captured:\n",
    "        q_values = self.model.predict(state)\n",
    "    return np.argmax(q_values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:15.088289200Z",
     "start_time": "2023-05-26T16:37:15.049348300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method remember to the DQNAgent class. It stores the agent's experience tuple (state, action, reward, next_state, done) in the memory. If the memory exceeds the specified memory size, the oldest experiences are removed to maintain the memory size."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def remember(self, state, action, reward, next_state, done):\n",
    "    state = state[0]\n",
    "    next_state = next_state[0] if next_state is not None else None\n",
    "    reward = np.clip(reward, -1, 1)\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "    if len(self.memory) > self.memory_size:\n",
    "        self.memory = self.memory[-self.memory_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:15.129060400Z",
     "start_time": "2023-05-26T16:37:15.098485600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method replay to the DQNAgent class. It performs the model training using the experience replay technique. It samples a batch of experiences from the memory and calculates the target Q-values using the Bellman equation. The model is then updated using the states and target Q-values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def replay(self, gamma):\n",
    "    if len(self.memory) < self.batch_size:\n",
    "        return\n",
    "    batch = random.sample(self.memory, self.batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    next_states = np.array(next_states) if any(x is not None for x in next_states) else None\n",
    "    dones = np.array(dones)\n",
    "\n",
    "    not_dones = 1 - dones\n",
    "    with io.capture_output() as captured:\n",
    "        targets = rewards + gamma * np.max(self.target_model.predict(next_states), axis=1) * not_dones\n",
    "        target_f = self.model.predict(states)\n",
    "    target_f[np.arange(self.batch_size), actions] = targets\n",
    "    self.model.train_on_batch(states, target_f)\n",
    "\n",
    "    self.steps += 1\n",
    "    if self.steps % self.update_frequency == 0:\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:15.169245800Z",
     "start_time": "2023-05-26T16:37:15.129060400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method train to the DQNAgent class. It trains the DQN agent for a specified number of episodes. It uses a progress bar to track the progress of the training. In each episode, it calls the run_episode method to run a single episode and update the model's weights. The epsilon value is decayed over episodes to gradually shift from exploration to exploitation. After training, the model is saved to a file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train(\n",
    "        self,\n",
    "        model_name,\n",
    "        episodes,\n",
    "        epsilon_decay,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.1,\n",
    "        gamma=0.99,\n",
    "        max_episode_length=1000\n",
    "):\n",
    "    epsilon = epsilon_start\n",
    "    bar_format = 'Training: {percentage:3.0f}% |{bar}| Elapsed: {elapsed} Remaining: {remaining}{postfix}'\n",
    "    training_pbar = tqdm(total=episodes, bar_format=bar_format, unit='episode')\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        total_reward = self.run_episode(epsilon, gamma, max_episode_length)\n",
    "        training_pbar.set_postfix_str(f'Reward: {int(total_reward)}')\n",
    "        training_pbar.update(1)\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        self.replay(gamma)\n",
    "\n",
    "    training_pbar.close()\n",
    "    print('Training completed.')\n",
    "    self.model.save(model_name)\n",
    "    print('Model saved.')\n",
    "    self.env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:15.208987300Z",
     "start_time": "2023-05-26T16:37:15.179062100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method run_episode to the DQNAgent class. It runs a single episode of the environment using the current policy and updates the model's weights. It iteratively selects actions, observes the next state and reward, and performs a model update using the Q-learning algorithm. The progress is tracked using a separate progress bar."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def run_episode(self, epsilon, gamma, max_episode_length):\n",
    "    observation = self.env.reset()\n",
    "    state = preprocess_frame(observation)\n",
    "    state = np.reshape(state, (1, *INPUT_SHAPE))\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_length = 0\n",
    "\n",
    "    while not done:\n",
    "        # if 'render_fps' in self.env.metadata:\n",
    "        #     self.env.render()\n",
    "        action = self.get_action(state, epsilon)\n",
    "        next_observation, reward, done, _, _ = self.env.step(action)\n",
    "        next_state = preprocess_frame(next_observation)\n",
    "        if next_state is not None:\n",
    "            next_state = np.reshape(next_state, (1, *INPUT_SHAPE))\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward # + 0.01\n",
    "            episode_length += 1\n",
    "        if episode_length >= max_episode_length or next_state is None:\n",
    "            done = True\n",
    "    return total_reward"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:15.248338Z",
     "start_time": "2023-05-26T16:37:15.208987300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defines the preprocess_frame function, which takes an observation from the environment and preprocesses the frame. It converts the frame to grayscale, resizes it to (84, 84) pixels, and normalizes the pixel values to the range [0, 1]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame[0]\n",
    "    if frame.ndim > 2:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = cv2.resize(frame, (84, 84)) / 255.0\n",
    "    return frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:15.289174900Z",
     "start_time": "2023-05-26T16:37:15.261034200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To open and play with the trained models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def open_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    env = gym.make(GAME_NAME, render_mode='human')\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    state = preprocess_frame(observation)\n",
    "    state = np.reshape(state, (1, *INPUT_SHAPE))\n",
    "\n",
    "    while not done:\n",
    "        if 'render_fps' in env.metadata:\n",
    "            env.render()\n",
    "        with io.capture_output() as captured:\n",
    "            action = np.argmax(model.predict(state))\n",
    "        next_observation, reward, done, _, _ = env.step(action)\n",
    "        next_state = preprocess_frame(next_observation)\n",
    "        next_state = np.reshape(next_state, (1, *INPUT_SHAPE))\n",
    "        state = next_state"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T16:37:15.328908100Z",
     "start_time": "2023-05-26T16:37:15.299054100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creates an instance of the DQNAgent class. It specifies the input shape, action space size, and learning rate for the agent. Trains the agent by calling the train method. It specifies the model name for saving, the number of episodes, epsilon decay rate, and maximum episode length. After training, it closes the environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    agent = DQNAgent()\n",
    "    agent.train(\n",
    "        model_name='trained_modell.h5',\n",
    "        episodes=150,\n",
    "        epsilon_decay=0.995,\n",
    "        max_episode_length=1000\n",
    "    )\n",
    "\n",
    "open_model('trained_modell.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-26T16:37:15.328908100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: Keep in mind that observation space for this environment are frames from environment. Observation space is an image of size (210, 160, 3). so you will need to implement an agent which can process images!(a CNN based agent). \n",
    "\n",
    "Make sure to do perform preprocessing on the frames. For example, you can convert the RBG image to gray. you can use [OpenCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) library to perform resize\\ing, bluring or any applicable filtering on the frames."
   ],
   "metadata": {
    "id": "3CYO6rorZlrC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grading criteria\n",
    "Project: 35 points\n",
    "\n",
    "* Final Viva: 10 points\n",
    "* Implementation: 10 points\n",
    "* Final Report: 15 points\n",
    "\n",
    "For viva you will need to expilictly mention each team member's contribution.\n",
    "\n",
    "You can write your report on this notebook. The report must include visualization of your results. Train your model at least with 2 different sets of hyperparameters and in visualization section compare their output.\n"
   ],
   "metadata": {
    "id": "nUJd6RS3dot5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good Luck!"
   ],
   "metadata": {
    "id": "LjLQtO8UdtEj"
   }
  }
 ]
}
