{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "rl",
   "language": "python",
   "display_name": "RL"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL Final Project\n",
    "\n",
    "Now it's finally time to put into use what we have learned so far in this course!\n",
    "\n",
    "The aim of this project is to assess your practical knowledge in Reinforcement Learning.\n",
    "\n",
    "your project consist of 2 parts. you will get the chance to work with 2 different environment.\n"
   ],
   "metadata": {
    "id": "AQWaHq-KLtHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Atari Game Pong"
   ],
   "metadata": {
    "id": "TdA4hr4kR33L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"zzzzzzzzzzzzzzzzzc\"/>"
   ],
   "metadata": {
    "id": "bS8EasNeaVx-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[Pong](https://www.gymlibrary.dev/environments/atari/pong/)** is a famus atari game that almost all of us have played it at least once!\n",
    "The goal of this task is to get engage with **gym** library and use Deep Reinforcement Learning to train an agent which can actually play this game!"
   ],
   "metadata": {
    "id": "lD3mZJkBWGxp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n",
    "\n",
    "!pip install ALE gym\n",
    "!pip install 'gym[accept-rom-license, atari]'\n",
    "!pip install opencv-python\n",
    "\n",
    "!pip install 'tensorflow<2.11'\n",
    "!pip install tensorflow-gpu==2.10\n",
    "\n",
    "!pip install tqdm\n",
    "!pip install jdc"
   ],
   "metadata": {
    "id": "nq9-gTd4Whko"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports the necessary libraries and modules for the code, including Gym (for the RL environment), OpenCV (for image processing), NumPy (for numerical operations), TensorFlow (for deep learning), and Keras (for building and training the DQN model)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import random\n",
    "import jdc\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.utils import io\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "id": "p6T-o8XRQ54i",
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:27.703250400Z",
     "start_time": "2023-05-22T20:44:21.845639100Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define important constant"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84, 1)\n",
    "TRAIN = True\n",
    "GAME_NAME = 'ALE/Pong-v5'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:29.173448700Z",
     "start_time": "2023-05-22T20:44:29.164450800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Supress unimportant warnings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:30.807559800Z",
     "start_time": "2023-05-22T20:44:30.796561400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sets up the TensorFlow session to run on the GPU. It configures the session with GPU options and sets it as the backend for Keras."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19596\\1512547178.py:4: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:35.968932900Z",
     "start_time": "2023-05-22T20:44:32.760662500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defines the preprocess_frame function, which takes an observation from the environment and preprocesses the frame. It converts the frame to grayscale, resizes it to (84, 84) pixels, and normalizes the pixel values to the range [0, 1]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame[0]\n",
    "    if frame.ndim > 2:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.resize(frame, (84, 84)) / 255.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:43.017514100Z",
     "start_time": "2023-05-22T20:44:43.008724300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defines the DQNAgent class, which represents the DQN (Deep Q-Network) agent. The DQNAgent has methods to build the model, get an action, store experiences, replay experiences for training, and run episodes. The model is built using the Keras Sequential API and consists of convolutional layers, fully connected layers, and an output layer. The model is compiled with the Adam optimizer and mean squared error (MSE) loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, learning_rate=0.00025, batch_size=32, memory_size=10000):\n",
    "        self.env = gym.make(GAME_NAME)\n",
    "        self.action_space = self.env.action_space.n\n",
    "        self.model = self.build_model(learning_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "    def build_model(self, learning_rate, activation='relu'):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(8, 8), strides=4, activation=activation, input_shape=INPUT_SHAPE))\n",
    "        model.add(Conv2D(64, kernel_size=(4, 4), strides=2, activation=activation))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), strides=1, activation=activation))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation=activation))\n",
    "        model.add(Dense(self.action_space))\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:44.984676700Z",
     "start_time": "2023-05-22T20:44:44.965420800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method get_action to the DQNAgent class. It takes a state and an epsilon value for epsilon-greedy exploration. With probability epsilon, it selects a random action. Otherwise, it uses the model to predict the Q-values for the given state and selects the action with the highest Q-value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def get_action(self, state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.randint(self.action_space)\n",
    "    with io.capture_output() as captured:\n",
    "        q_values = self.model.predict(state)\n",
    "    return np.argmax(q_values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:46.682676800Z",
     "start_time": "2023-05-22T20:44:46.655869300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method remember to the DQNAgent class. It stores the agent's experience tuple (state, action, reward, next_state, done) in the memory. If the memory exceeds the specified memory size, the oldest experiences are removed to maintain the memory size."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def remember(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "    if len(self.memory) > self.memory_size:\n",
    "        self.memory = self.memory[-self.memory_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:47.937499Z",
     "start_time": "2023-05-22T20:44:47.917154900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method replay to the DQNAgent class. It performs the model training using the experience replay technique. It samples a batch of experiences from the memory and calculates the target Q-values using the Bellman equation. The model is then updated using the states and target Q-values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def replay(self, gamma):\n",
    "    if len(self.memory) < self.batch_size:\n",
    "        return\n",
    "    batch = random.sample(self.memory, self.batch_size)\n",
    "    states = []\n",
    "    targets = []\n",
    "    for state, action, reward, next_state, done in batch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            q_values = self.model.predict(next_state)\n",
    "            target += gamma * np.max(q_values[0])\n",
    "        target_f = self.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        states.append(state)\n",
    "        targets.append(target_f)\n",
    "\n",
    "    states = np.array([state[0] for state in states])\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    self.model.fit(states, targets, batch_size=self.batch_size, verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:49.250486400Z",
     "start_time": "2023-05-22T20:44:49.231527700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method train to the DQNAgent class. It trains the DQN agent for a specified number of episodes. It uses a progress bar to track the progress of the training. In each episode, it calls the run_episode method to run a single episode and update the model's weights. The epsilon value is decayed over episodes to gradually shift from exploration to exploitation. After training, the model is saved to a file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train(self, model_name, episodes, epsilon_decay, epsilon_start=1.0, epsilon_end=0.1, gamma=0.97,\n",
    "          max_episode_length=100):\n",
    "    epsilon = epsilon_start\n",
    "    bar_format = 'Training: {percentage:3.0f}% |{bar}| Elapsed: {elapsed}{postfix}'\n",
    "    training_pbar = tqdm(total=episodes, bar_format=bar_format, unit='episode')\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        total_reward = self.run_episode(epsilon, gamma, max_episode_length)\n",
    "        training_pbar.set_postfix_str(f'Reward: {total_reward}')\n",
    "        training_pbar.update(1)\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        with io.capture_output() as captured:\n",
    "            self.replay(gamma)\n",
    "\n",
    "    training_pbar.close()\n",
    "    print('Training completed.')\n",
    "\n",
    "    self.model.save(model_name)\n",
    "    print('Model saved.')\n",
    "\n",
    "    self.env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:51.293070200Z",
     "start_time": "2023-05-22T20:44:51.281669400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adds a method run_episode to the DQNAgent class. It runs a single episode of the environment using the current policy and updates the model's weights. It iteratively selects actions, observes the next state and reward, and performs a model update using the Q-learning algorithm. The progress is tracked using a separate progress bar."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def run_episode(self, epsilon, gamma, max_episode_length):\n",
    "    observation = self.env.reset()\n",
    "    state = preprocess_frame(observation)\n",
    "    state = np.reshape(state, (1, *INPUT_SHAPE))\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_length = 0\n",
    "\n",
    "    # bar_format = 'Episode: {percentage:3.0f}% |{bar}| Speed: {rate_fmt}{postfix}'\n",
    "    # episode_pbar = tqdm(total=max_episode_length, bar_format=bar_format, unit='step')\n",
    "\n",
    "    while not done:\n",
    "        action = self.get_action(state, epsilon)\n",
    "        next_observation, reward, done, _, _ = self.env.step(action)\n",
    "        next_state = preprocess_frame(next_observation)\n",
    "        if next_state is not None:\n",
    "            next_state = np.reshape(next_state, (1, *INPUT_SHAPE))\n",
    "            with io.capture_output() as captured:\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "            # episode_pbar.set_postfix_str(f'Reward: {total_reward}')\n",
    "            # episode_pbar.update(1)\n",
    "\n",
    "        if episode_length >= max_episode_length:\n",
    "            done = True\n",
    "\n",
    "    # episode_pbar.close()\n",
    "    return total_reward"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:44:53.332795100Z",
     "start_time": "2023-05-22T20:44:53.295585700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creates an instance of the DQNAgent class. It specifies the input shape, action space size, and learning rate for the agent. Trains the agent by calling the train method. It specifies the model name for saving, the number of episodes, epsilon decay rate, and maximum episode length. After training, it closes the environment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0% |          | Elapsed: 00:00, Remaining: ?",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0f14e8228214f11a05bf32370b332dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    agent = DQNAgent()\n",
    "\n",
    "    agent.train(\n",
    "        model_name='trained_model.h5',\n",
    "        episodes=100,\n",
    "        epsilon_decay=0.99,\n",
    "        max_episode_length=1000\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:41:48.076846700Z",
     "start_time": "2023-05-22T20:08:10.915241700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To open and play with the trained models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 19\u001B[0m\n\u001B[0;32m     15\u001B[0m         next_state \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(next_state, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39mINPUT_SHAPE))\n\u001B[0;32m     16\u001B[0m         state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[1;32m---> 19\u001B[0m \u001B[43mopen_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrained_model.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[19], line 13\u001B[0m, in \u001B[0;36mopen_model\u001B[1;34m(model_name)\u001B[0m\n\u001B[0;32m     11\u001B[0m env\u001B[38;5;241m.\u001B[39mrender()\n\u001B[0;32m     12\u001B[0m action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(model\u001B[38;5;241m.\u001B[39mpredict(state))\n\u001B[1;32m---> 13\u001B[0m next_observation, reward, done, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m next_state \u001B[38;5;241m=\u001B[39m preprocess_frame(next_observation)\n\u001B[0;32m     15\u001B[0m next_state \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mreshape(next_state, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39mINPUT_SHAPE))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\ale_py\\env\\gym.py:256\u001B[0m, in \u001B[0;36mAtariEnv.step\u001B[1;34m(self, action_ind)\u001B[0m\n\u001B[0;32m    254\u001B[0m reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(frameskip):\n\u001B[1;32m--> 256\u001B[0m     reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39male\u001B[38;5;241m.\u001B[39mact(action)\n\u001B[0;32m    257\u001B[0m is_terminal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39male\u001B[38;5;241m.\u001B[39mgame_over(with_truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    258\u001B[0m is_truncated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39male\u001B[38;5;241m.\u001B[39mgame_truncated()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def open_model(model_name):\n",
    "    model = load_model(model_name)\n",
    "    env = gym.make(GAME_NAME, render_mode='human')\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    state = preprocess_frame(observation)\n",
    "    state = np.reshape(state, (1, *INPUT_SHAPE))\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = np.argmax(model.predict(state))\n",
    "        next_observation, reward, done, _, _ = env.step(action)\n",
    "        next_state = preprocess_frame(next_observation)\n",
    "        next_state = np.reshape(next_state, (1, *INPUT_SHAPE))\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "open_model('trained_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T20:43:35.132871300Z",
     "start_time": "2023-05-22T20:43:09.194646200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: Keep in mind that observation space for this environment are frames from environment. Observation space is an image of size (210, 160, 3). so you will need to implement an agent which can process images!(a CNN based agent). \n",
    "\n",
    "Make sure to do perform preprocessing on the frames. For example, you can convert the RBG image to gray. you can use [OpenCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) library to perform resize\\ing, bluring or any applicable filtering on the frames."
   ],
   "metadata": {
    "id": "3CYO6rorZlrC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grading criteria\n",
    "Project: 35 points\n",
    "\n",
    "* Final Viva: 10 points\n",
    "* Implementation: 10 points\n",
    "* Final Report: 15 points\n",
    "\n",
    "For viva you will need to expilictly mention each team member's contribution.\n",
    "\n",
    "You can write your report on this notebook. The report must include visualization of your results. Train your model at least with 2 different sets of hyperparameters and in visualization section compare their output.\n"
   ],
   "metadata": {
    "id": "nUJd6RS3dot5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good Luck!"
   ],
   "metadata": {
    "id": "LjLQtO8UdtEj"
   }
  }
 ]
}
