{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "rl",
   "language": "python",
   "display_name": "RL"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL Final Project\n",
    "\n",
    "Now it's finally time to put into use what we have learned so far in this course!\n",
    "\n",
    "The aim of this project is to assess your practical knowledge in Reinforcement Learning.\n",
    "\n",
    "your project consist of 2 parts. you will get the chance to work with 2 different environment.\n"
   ],
   "metadata": {
    "id": "AQWaHq-KLtHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Atari Game Pong"
   ],
   "metadata": {
    "id": "TdA4hr4kR33L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"zzzzzzzzzzzzzzzzzc\"/>"
   ],
   "metadata": {
    "id": "bS8EasNeaVx-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[Pong](https://www.gymlibrary.dev/environments/atari/pong/)** is a famus atari game that almost all of us have played it at least once!\n",
    "The goal of this task is to get engage with **gym** library and use Deep Reinforcement Learning to train an agent which can actually play this game!"
   ],
   "metadata": {
    "id": "lD3mZJkBWGxp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install ALE\n",
    "# !pip install gym\n",
    "# !pip install opencv-python\n",
    "#\n",
    "# !pip install \"tensorflow==2.10\"\n",
    "# !pip install \"tensorflow-gpu==2.10\"\n",
    "#\n",
    "# !pip install tqdm\n",
    "# !pip install jdc\n",
    "#\n",
    "# !pip list"
   ],
   "metadata": {
    "id": "nq9-gTd4Whko"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import cv2\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from IPython.utils import io\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "id": "p6T-o8XRQ54i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "GAME_NAME = 'ALE/Pong-v5'\n",
    "MODEL_PATH = './pong-dqn.h5'\n",
    "BEST_MODEL_PATH = './best_model.h5'\n",
    "MODEL_ACTIVATION = 'relu'\n",
    "INPUT_SHAPE = (84, 84, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 10000\n",
    "\n",
    "GAMMA = 0.95\n",
    "EPSILON = 1.0\n",
    "MIN_EPSILON = 0.1\n",
    "EPSILON_DECAY = 0.995\n",
    "LEARNING_RATE = 0.001"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, size, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.priorities = deque(maxlen=size)\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(self.max_priority)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        probs = np.array(self.priorities) ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        beta = self.beta_start + (1 - self.beta_start) * (self.frame / self.beta_frames)\n",
    "        self.frame += 1\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        experiences = [self.buffer[i] for i in indices]\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return experiences, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, indices, errors, absolute_error=1e-5):\n",
    "        for i, error in zip(indices, errors):\n",
    "            priority = np.max(np.abs(error)) + absolute_error\n",
    "            self.priorities[i] = priority\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
    "        self.epsilon = EPSILON\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def build_model(self):\n",
    "        inputs = tf.keras.layers.Input(shape=self.state_size)\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=3, activation=MODEL_ACTIVATION)(inputs)\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=3, activation=MODEL_ACTIVATION)(x)\n",
    "        x = tf.keras.layers.Conv2D(128, kernel_size=3, activation=MODEL_ACTIVATION)(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        fc1 = tf.keras.layers.Dense(256, activation=MODEL_ACTIVATION)(x)\n",
    "        value = tf.keras.layers.Dense(1)(fc1)\n",
    "        advantage = tf.keras.layers.Dense(self.action_size)(fc1)\n",
    "        q_values = value + (advantage - tf.math.reduce_mean(advantage, axis=1, keepdims=True))\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=q_values)\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def run_episode(self):\n",
    "        experiences, indices, weights = self.memory.sample(BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states = np.squeeze(states, axis=1)\n",
    "        next_states = np.squeeze(next_states, axis=1)\n",
    "        target_q_values = self.target_model.predict(np.array(next_states))\n",
    "        online_q_values = self.model.predict(np.array(next_states))\n",
    "        best_actions = np.argmax(online_q_values, axis=1)\n",
    "\n",
    "        targets = rewards + (1 - np.array(dones)) * GAMMA * target_q_values[np.arange(BATCH_SIZE), best_actions]\n",
    "        target_f = self.model.predict(np.array(states))\n",
    "        target_f[np.arange(BATCH_SIZE), actions] = targets\n",
    "\n",
    "        errors = np.abs(self.model.predict(np.array(states)) - target_f)\n",
    "        self.memory.update_priorities(indices, errors)\n",
    "\n",
    "        if self.epsilon > MIN_EPSILON:\n",
    "            self.epsilon *= EPSILON_DECAY\n",
    "\n",
    "        self.model.fit(states, target_f, verbose=0, callbacks=[best_checkpointer])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = frame[0]\n",
    "    if len(frame.shape) == 3 and frame.shape[2] == 3:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "    return np.reshape(resized, INPUT_SHAPE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(episodes):\n",
    "    env = gym.make(GAME_NAME)\n",
    "    state_size = INPUT_SHAPE\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    bar_format = 'Training: {percentage:3.0f}% |{bar}| Elapsed: {elapsed} Remaining: {remaining}{postfix}'\n",
    "    training_pbar = tqdm(total=episodes, bar_format=bar_format, unit='episode')\n",
    "\n",
    "    best_total_reward = -np.inf\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = preprocess_frame(env.reset())\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with io.capture_output() as captured:\n",
    "                action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            next_state = preprocess_frame(next_state)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if total_reward > best_total_reward:\n",
    "                    print(f\"New best total reward {total_reward}, saving model weights.\")\n",
    "                    best_total_reward = total_reward\n",
    "                    agent.model.save_weights(MODEL_PATH)\n",
    "\n",
    "        if len(agent.memory) > BATCH_SIZE:\n",
    "            with io.capture_output() as captured:\n",
    "                agent.run_episode()\n",
    "\n",
    "        training_pbar.set_postfix_str(f'Reward: {total_reward}')\n",
    "        training_pbar.update(1)\n",
    "\n",
    "    training_pbar.close()\n",
    "\n",
    "\n",
    "best_checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=BEST_MODEL_PATH,\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "train(episodes=200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def play_with_model():\n",
    "    env = gym.make(GAME_NAME, render_mode='human')\n",
    "    state_size = INPUT_SHAPE\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.load(BEST_MODEL_PATH)\n",
    "\n",
    "    state = preprocess_frame(env.reset())\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        with io.capture_output() as captured:\n",
    "            action = agent.choose_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = preprocess_frame(next_state)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "\n",
    "\n",
    "play_with_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: Keep in mind that observation space for this environment are frames from environment. Observation space is an image of size (210, 160, 3). so you will need to implement an agent which can process images!(a CNN based agent). \n",
    "\n",
    "Make sure to do perform preprocessing on the frames. For example, you can convert the RBG image to gray. you can use [OpenCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) library to perform resize\\ing, bluring or any applicable filtering on the frames."
   ],
   "metadata": {
    "id": "3CYO6rorZlrC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grading criteria\n",
    "Project: 35 points\n",
    "\n",
    "* Final Viva: 10 points\n",
    "* Implementation: 10 points\n",
    "* Final Report: 15 points\n",
    "\n",
    "For viva you will need to expilictly mention each team member's contribution.\n",
    "\n",
    "You can write your report on this notebook. The report must include visualization of your results. Train your model at least with 2 different sets of hyperparameters and in visualization section compare their output.\n"
   ],
   "metadata": {
    "id": "nUJd6RS3dot5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good Luck!"
   ],
   "metadata": {
    "id": "LjLQtO8UdtEj"
   }
  }
 ]
}
