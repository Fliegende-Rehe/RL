{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "rl",
   "language": "python",
   "display_name": "RL"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RL Final Project\n",
    "\n",
    "Now it's finally time to put into use what we have learned so far in this course!\n",
    "\n",
    "The aim of this project is to assess your practical knowledge in Reinforcement Learning.\n",
    "\n",
    "your project consist of 2 parts. you will get the chance to work with 2 different environment.\n"
   ],
   "metadata": {
    "id": "AQWaHq-KLtHi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.Atari Game Pong"
   ],
   "metadata": {
    "id": "TdA4hr4kR33L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"zzzzzzzzzzzzzzzzzc\"/>"
   ],
   "metadata": {
    "id": "bS8EasNeaVx-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[Pong](https://www.gymlibrary.dev/environments/atari/pong/)** is a famus atari game that almost all of us have played it at least once!\n",
    "The goal of this task is to get engage with **gym** library and use Deep Reinforcement Learning to train an agent which can actually play this game!"
   ],
   "metadata": {
    "id": "lD3mZJkBWGxp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install ALE\n",
    "# !pip install gym\n",
    "# !pip install opencv-python\n",
    "#\n",
    "# !pip install \"tensorflow==2.10\"\n",
    "# !pip install \"tensorflow-gpu==2.10\"\n",
    "#\n",
    "# !pip install tqdm\n",
    "# !pip install jdc\n",
    "#\n",
    "# !pip list"
   ],
   "metadata": {
    "id": "nq9-gTd4Whko",
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:16.334777Z",
     "start_time": "2023-05-26T19:17:16.319150300Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import cv2\n",
    "import jdc\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from IPython.utils import io\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "id": "p6T-o8XRQ54i",
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:19.450845200Z",
     "start_time": "2023-05-26T19:17:16.334777Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:19.466468Z",
     "start_time": "2023-05-26T19:17:19.450845200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84, 1)\n",
    "GAME_NAME = 'ALE/Pong-v5'\n",
    "BATCH_SIZE = 32\n",
    "MEMORY_SIZE = 2000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:19.482094900Z",
     "start_time": "2023-05-26T19:17:19.466468Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:19.544586600Z",
     "start_time": "2023-05-26T19:17:19.482094900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name,\n",
    "            action_space,\n",
    "            gamma=0.95,\n",
    "            epsilon=1.0, epsilon_min=0.01,\n",
    "            epsilon_decay=0.995,\n",
    "            learning_rate=0.001,\n",
    "            episodes=100\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.observation_space = INPUT_SHAPE\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        self.episodes = episodes\n",
    "        self.batch_size = BATCH_SIZE\n",
    "\n",
    "    def build_model(self, pad='same'):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Conv2D(32, (8, 8), strides=4, padding=pad, input_shape=INPUT_SHAPE))\n",
    "        model.add(tf.keras.layers.Conv2D(64, (4, 4), strides=2, padding=pad))\n",
    "        model.add(tf.keras.layers.Conv2D(64, (3, 3), strides=1, padding=pad))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_space, activation='linear'))\n",
    "        # tf.keras.losses.Huber()\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:19.591413100Z",
     "start_time": "2023-05-26T19:17:19.544586600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def preprocess_frame(self, frame):\n",
    "    frame = frame[0]\n",
    "    if len(frame.shape) == 3 and frame.shape[2] == 3:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(frame, (84, 84))\n",
    "    return resized"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:19.591413100Z",
     "start_time": "2023-05-26T19:17:19.560219300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "        return random.randrange(self.action_space)\n",
    "    with io.capture_output() as captured:\n",
    "        act_values = self.model.predict(state.reshape(1, 84, 84, 1))\n",
    "    return np.argmax(act_values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:19.591413100Z",
     "start_time": "2023-05-26T19:17:19.591413100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def replay(self, batch_size):\n",
    "    minibatch = random.sample(self.memory, batch_size)\n",
    "    states = np.array([x[0] for x in minibatch])\n",
    "    actions = np.array([x[1] for x in minibatch])\n",
    "    rewards = np.array([x[2] for x in minibatch])\n",
    "    next_states = np.array([x[3] for x in minibatch])\n",
    "    dones = np.array([x[4] for x in minibatch])\n",
    "\n",
    "    states = states.reshape(states.shape[0], *self.observation_space)\n",
    "    next_states = next_states.reshape(next_states.shape[0], *self.observation_space)\n",
    "    dones = np.array([x[4] for x in minibatch])\n",
    "\n",
    "    targets = rewards + self.gamma * np.amax(self.model.predict_on_batch(next_states), axis=1) * (1 - dones)\n",
    "    target_f = self.model.predict_on_batch(states)\n",
    "\n",
    "    for i, action in enumerate(actions):\n",
    "        target_f[i][action] = targets[i]\n",
    "\n",
    "    self.model.fit(states, target_f, epochs=1, verbose=0)\n",
    "\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:19.607064600Z",
     "start_time": "2023-05-26T19:17:19.591413100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "%%add_to DQNAgent\n",
    "\n",
    "def train(self, max_episode_length=1500):\n",
    "    bar_format = 'Training: {percentage:3.0f}% |{bar}| Elapsed: {elapsed} Remaining: {remaining}{postfix}'\n",
    "    training_pbar = tqdm(total=self.episodes, bar_format=bar_format, unit='episode')\n",
    "    for e in range(self.episodes):\n",
    "        state = env.reset()\n",
    "        state = self.preprocess_frame(state)\n",
    "        state = np.reshape(state, [1, 84, 84, 1])\n",
    "        total_reward = 0\n",
    "        for time in range(max_episode_length):\n",
    "            action = self.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = self.preprocess_frame(next_state)\n",
    "            next_state = np.reshape(next_state, [1, 84, 84, 1])\n",
    "            self.memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "            total_reward += reward\n",
    "\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.replay(self.batch_size)\n",
    "\n",
    "        if e % 100 == 0:\n",
    "            self.model.save(self.model_name)\n",
    "\n",
    "        training_pbar.set_postfix_str(f'Reward: {total_reward}')\n",
    "        training_pbar.update(1)\n",
    "\n",
    "    training_pbar.close()\n",
    "    print('Training completed')\n",
    "    env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T19:17:19.631862700Z",
     "start_time": "2023-05-26T19:17:19.607064600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0% |          | Elapsed: 00:00 Remaining: ?",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ffa22cff7f542efade79557dba4c2d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_22480\\1713522639.py\", line 7, in <module>\n",
      "    agent.train()\n",
      "  File \"<string>\", line 24, in train\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend.py\", line 4240, in <listcomp>\n",
      "    return [x.numpy() for x in tensors]\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 15.1 MiB for an array with shape (7744, 512) and data type float32\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\executing\\executing.py\", line 317, in executing\n",
      "    args = executing_cache[key]\n",
      "KeyError: (<code object <listcomp> at 0x0000028AAEE0F100, file \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\keras\\backend.py\", line 4240>, 2794662719744, 12)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 854, in get_records\n",
      "    return list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\stack_data\\core.py\", line 565, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\stack_data\\utils.py\", line 84, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\stack_data\\core.py\", line 555, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\stack_data\\core.py\", line 520, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\executing\\executing.py\", line 369, in executing\n",
      "    args = find(source=cls.for_frame(frame), retry_cache=True)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\executing\\executing.py\", line 252, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\executing\\executing.py\", line 270, in for_filename\n",
      "    result = source_cache[filename] = cls._for_filename_and_lines(filename, lines)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\executing\\executing.py\", line 281, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\stack_data\\core.py\", line 81, in __init__\n",
      "    self.asttokens()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\executing\\executing.py\", line 413, in asttokens\n",
      "    return ASTTokens(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\asttokens\\asttokens.py\", line 59, in __init__\n",
      "    self._tokens = list(self._generate_tokens(source_text))\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\RL\\lib\\site-packages\\asttokens\\asttokens.py\", line 85, in _generate_tokens\n",
      "    for index, tok in enumerate(tokenize.generate_tokens(io.StringIO(text).readline)):\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(GAME_NAME)\n",
    "agent = DQNAgent(\n",
    "    model_name='trained_model.h5',\n",
    "    action_space=env.action_space.n,\n",
    "    episodes=200\n",
    ")\n",
    "agent.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T20:09:35.283183400Z",
     "start_time": "2023-05-26T19:17:19.631862700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: Keep in mind that observation space for this environment are frames from environment. Observation space is an image of size (210, 160, 3). so you will need to implement an agent which can process images!(a CNN based agent). \n",
    "\n",
    "Make sure to do perform preprocessing on the frames. For example, you can convert the RBG image to gray. you can use [OpenCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html) library to perform resize\\ing, bluring or any applicable filtering on the frames."
   ],
   "metadata": {
    "id": "3CYO6rorZlrC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grading criteria\n",
    "Project: 35 points\n",
    "\n",
    "* Final Viva: 10 points\n",
    "* Implementation: 10 points\n",
    "* Final Report: 15 points\n",
    "\n",
    "For viva you will need to expilictly mention each team member's contribution.\n",
    "\n",
    "You can write your report on this notebook. The report must include visualization of your results. Train your model at least with 2 different sets of hyperparameters and in visualization section compare their output.\n"
   ],
   "metadata": {
    "id": "nUJd6RS3dot5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Good Luck!"
   ],
   "metadata": {
    "id": "LjLQtO8UdtEj"
   }
  }
 ]
}
